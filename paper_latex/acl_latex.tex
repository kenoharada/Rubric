\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[review]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{colortbl}
\usepackage[most]{tcolorbox}
\definecolor{RoyalBlue}{RGB}{65,105,225}
\colorlet{White}{white}
\newcommand{\procname}[1]{\textsc{#1}}
\newcommand{\rubric}{\mathrm{rubric}}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Automated Refinement of Essay Scoring Rubrics \\for Language Models via Reflect-and-Revise}

\author{
 \textbf{Keno Harada},
 \textbf{Lui Yoshida},
 \textbf{Takeshi Kojima},
 \textbf{Yusuke Iwasawa},
 \textbf{Yutaka Matsuo}
\\
The University of Tokyo
\\
 \small{
 \texttt{{keno.harada@weblab.t.u-tokyo.ac.jp}}}
 }

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\begin{abstract}
Large Language Models (LLMs) are increasingly used for Automated Essay Scoring (AES), yet the scoring rubrics they rely on are typically designed for human raters and may not be optimal for LLMs. Inspired by the calibration process that human raters undergo before formal scoring, we propose Reflect-and-Revise, an iterative framework that refines scoring rubrics by prompting models to reflect on their own chain-of-thought rationales and score discrepancies with human labels. At each iteration, the model identifies systematic error patterns from sampled mismatches and revises the rubric accordingly. Experiments on three essay scoring benchmarks (ASAP, ASAP 2.0, and TOEFL11) with three LLMs (GPT-5 mini, Gemini 3 Flash, and Qwen3-80B-A3B) demonstrate that our method yields substantial improvements in Quadratic Weighted Kappa (QWK), achieving gains of up to +0.438 over human-authored rubrics. Starting from a minimal seed rubric that specifies only the score scale, our method matches or exceeds expert rubric performance in seven out of nine dataset-model combinations, indicating that iterative refinement can reduce the manual effort of rubric authoring. Analysis of the refined rubrics reveals that the optimization process introduces explicit procedural structures, such as conditional gating rules and quantitative thresholds, that are absent from human-authored rubrics, highlighting a gap between rubrics designed for human raters and those effective for LLMs.\footnote{Refinement and evaluation codes are available at \url{https://anonymous.4open.science/r/RubricCode-C333}}
\end{abstract}

\section{Introduction}
Automated Essay Scoring (AES) systems powered by Large Language Models (LLMs) are increasingly expected to provide real-time, scalable feedback for students and alleviate the grading burden on instructors~\citep{mizumoto2023aes,yancey-etal-2023-rating,naismith-etal-2023-automated,pack2024validity}. Typically, these systems employ static, pre-defined rubrics to guide the evaluation. However, it remains an open question whether rubrics designed for human raters are optimal for LLMs. When human raters use a rubric, they often engage in a collaborative calibration process: they score sample essays, discuss discrepancies in their judgments, and refine their shared understanding of the criteria to ensure consistency~\citep{trace2016rubricsnegotiate,ozfidan2022rubricdev,yoo-etal-2025-dress}. This iterative, reflective practice is overlooked in current LLM-based AES, potentially limiting their alignment with human scoring patterns.

Recent studies show that LLMs have the ability to refine their own outputs especially when there is reliable external feedback~\citep{madaan2023selfrefine,kamoi2024selfcorrect}. Prompt optimization techniques leverage these capabilities to update prompts to maximize a targeted metric and show performance improvements in various tasks such as multi-hop reasoning, instruction following and privacy-aware delegation~\citep{khattab2023dspy,opsahl-ong-etal-2024-optimizing,agrawal2025gepa}.

Inspired by these developments and the calibration process of human raters, we propose an iterative Reflect-and-Revise approach for refining rubrics in LLM-based AES.
Specifically, given a small set of 100 sample essays with human scores, the model iteratively refines the rubric by reflecting on its own scoring rationales and the discrepancies between its predicted scores and human labels, with the objective of maximizing Quadratic Weighted Kappa (QWK) between model and human scores.

Our method differs from prior rubric refinement approaches~\citep{xie2024gradelikehuman,lee-etal-2024-unleashing,liu-etal-2024-calibrating} in two respects, both motivated by the improved reasoning and instruction-following capabilities of recent LLMs. First, we incorporate the model's chain-of-thought rationales into the revision process, providing richer diagnostic signals for identifying why the current rubric leads to scoring errors. Second, we apply iterative refinement over multiple rounds, enabling progressive rubric improvement rather than relying on a single revision pass. 

We evaluate on three datasets (ASAP, ASAP 2.0, and TOEFL11) with three models (\texttt{GPT-5 mini}, \texttt{Qwen3-80B-A3B}, and \texttt{Gemini 3 Flash}). Our method consistently outperforms both human-authored rubrics and the AutoCalibrate baseline~\citep{liu-etal-2024-calibrating} on ASAP and ASAP 2.0, achieving QWK gains of up to +0.438 over human rubrics. Starting from a minimal seed rubric that specifies only the score scale, our method matches or exceeds the performance of expert rubrics in seven out of nine dataset--model combinations. Ablation studies confirm that both components contribute to the observed improvements. Furthermore, analysis of the refined rubrics reveals that the refinement process introduces explicit procedural structures, such as conditional gating rules and quantitative thresholds, that are absent from human-authored rubrics.

\begin{table*}[t]
  \centering
  \resizebox{\textwidth}{!}{%
  \begin{tabular}{lcccccc}
    \toprule
    \textbf{Study} &
    \textbf{Scores} &
    \textbf{Texts} &
    \textbf{Sources} &
    \textbf{Predictions} &
    \textbf{Rationales} &
    \textbf{Iterative} \\
    \midrule
    HD-Eval (\citet{liu-etal-2024-hd}) & $\checkmark$ &  &  &  &  &  $\checkmark$\\
    MTS (\citet{lee-etal-2024-unleashing}) &  &  & $\checkmark$ &  &  &  \\
    \citet{xie2024gradelikehuman} & $\checkmark$ & $\checkmark$ &  &  &  &  \\
    ActiveCritic (\citet{xu2025activecritics}) & $\checkmark$ & $\checkmark$ & $\checkmark$ &  &  &  \\
    AutoCalibrate (\citet{liu-etal-2024-calibrating}) & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$ &  &  \\
    \midrule
    \textbf{Ours} &
    \textbf{$\checkmark$} & \textbf{$\checkmark$} & \textbf{$\checkmark$} & \textbf{$\checkmark$} & \textbf{$\checkmark$} & \textbf{$\checkmark$} \\
    \bottomrule
  \end{tabular}}
  \caption{Comparison of signals and strategies used to refine evaluation rubrics. \textbf{Scores}: whether the method uses human-labeled scores. \textbf{Texts}: whether the method uses the texts being evaluated (essays/answers/responses that receive scores). \textbf{Sources}: whether the method uses source passages used to compose responses (e.g., source documents, provided essay themes). \textbf{Predictions}: whether the method uses model-predicted scores. \textbf{Rationales}: whether the method uses model-generated justifications accompanying predicted scores. \textbf{Iterative}: whether the method iteratively refines rubrics over multiple rounds. Our method uniquely leverages all five signals and performs iterative refinement.}
\label{tab:rubric_rewrite_data_requirements}
\end{table*}

\section{Related Work}
For non-verifiable tasks, where judging success is not as straightforward as in math or code, recent research has focused on LLM-based automatic evaluation using checklists and rubrics in prompts~\citep{min-etal-2023-factscore,qin-etal-2024-infobench,lin2024wildbench,wu-etal-2025-lifbench,cook2025ticking,huang2025rubricanchors,gunjal2025rubricsasrewards,viswanathan2025checklistsbetterrewardmodels,lee2025checkeval,xu2025activecritics,liu-etal-2024-calibrating,liu-etal-2024-hd,wen-etal-2025-hpss}. AES is an example of such a non-verifiable task, and various techniques have been proposed~\citep{mizumoto2023aes,xie2024gradelikehuman,lee-etal-2024-unleashing}.

\subsection{Rubric Design for LLM Evaluation}
Recent studies suggest that the relationship between rubric design and LLM evaluation quality is not straightforward. \citet{yoshida2025rubrics} found that making rubrics more detailed does not always lead to performance gains in AES: three out of four models maintained similar scoring accuracy with a simplified rubric, and one model even showed decreased performance with more detailed rubrics. Similarly, \citet{furuhashi2025checklist} identified ``negative items,'' rubric components that are valid for human evaluators but do not improve LLM performance, and showed that removing such items can even boost accuracy. These findings suggest that there remains room to find rubric formulations better suited for LLMs.

\subsection{LLM-based Rubric Refinement}
A growing body of work explores methods for generating or refining evaluation rubrics to improve agreement with human scores~\citep{liu-etal-2024-hd,lee-etal-2024-unleashing,xie2024gradelikehuman,xu2025activecritics,liu-etal-2024-calibrating}. Some methods generate rubrics in a single pass without subsequent revision: for example, generating rubrics from source passages~\citep{lee-etal-2024-unleashing}, from few input--score examples~\citep{xu2025activecritics}, or by rewriting existing rubrics using human-labeled scores and evaluated texts~\citep{xie2024gradelikehuman}. 

Among these, \citet{liu-etal-2024-calibrating} proposed the pipeline closest to ours. Their method samples candidate evaluation criteria from human-scored data, scores a held-out set, and refines the best-performing criteria by analyzing error cases where model scores diverge from human labels. However, their approach differs from ours in two key respects: (1) the refinement loop is executed only once rather than iteratively, and (2) the refinement step does not incorporate the model's chain-of-thought rationales, limiting the diagnostic signal available for rubric revision.

Our method extends this line of work by enabling the LLM to reflect on its own scoring output, including its chain-of-thought rationales, to iteratively refine the rubric. By feeding back not only human-labeled and model-predicted scores but also the model's justifications for those predictions, our approach provides richer diagnostic information for identifying why the current rubric leads to scoring errors. This process effectively mimics the calibration sessions of human evaluators, who refine their interpretations and build shared understanding before formal scoring~\citep{trace2016rubricsnegotiate,ozfidan2022rubricdev,yoo-etal-2025-dress}. Table~\ref{tab:rubric_rewrite_data_requirements} summarizes the signals used for rubric refinement across prior work and our method.

\input{combined_table.tex}


\section{Iterative Rubric Refinement}
\label{sec:method}
Our method iteratively refines rubric text from score mismatches between LLM predictions and human labels, and model's chain-of-thought rationales. We provide the full algorithm in Appendix~\ref{sec:appendix}.

\subsection{Preliminaries}
Let \(\mathcal{D}_{\mathrm{train}}=\{(x_i,y_i)\}_{i=1}^{N}\) be the training set, where \(x_i\) is an essay response and \(y_i\) is the corresponding human score. Rubric search starts from an initial seed rubric \(\rubric_0\) (from coarse to detailed variants) and iteratively updates it over \(T\) iterations, maintaining a candidate pool of at most \(K\) rubrics. At each iteration, \(M\) Monte Carlo trials are run per candidate, each drawing error batches of sizes \(b \in \mathcal{B}\).

Given a rubric \(\rubric\), the evaluator LLM scores each training essay \(x_i\), producing a predicted score \(\hat{y}_i\) and a chain-of-thought rationale \(z_i\):
\begin{equation}
(\hat{y}_i,\, z_i) = \mathrm{LLM}(\rubric,\, x_i).
\end{equation}
We measure rubric quality by Quadratic Weighted Kappa (QWK) between the predicted scores \(\hat{\mathbf{y}}_{\rubric}=(\hat{y}_1,\dots,\hat{y}_N)\) and the human scores \(\mathbf{y}=(y_1,\dots,y_N)\), and seek:
\begin{equation}
\rubric_{\mathrm{best}} = \arg\max_{\rubric}\;\mathrm{QWK}(\hat{\mathbf{y}}_{\rubric},\,\mathbf{y}).
\end{equation}
For brevity, we hereafter write \(\mathrm{QWK}(\rubric)\) to denote \(\mathrm{QWK}(\hat{\mathbf{y}}_{\rubric},\mathbf{y})\) on the training set. QWK is a standard metric in automated essay scoring for measuring agreement with human raters~\citep{ijcai2019p879}. It is an agreement metric for ordinal labels and penalizes larger score gaps more heavily than smaller ones:
\begin{equation}
\mathrm{QWK} = 1 - \frac{\sum_{i,j} w_{ij} O_{ij}}{\sum_{i,j} w_{ij} E_{ij}}, \quad
w_{ij}=\frac{(i-j)^2}{(L-1)^2},
\end{equation}
where \(O_{ij}\) and \(E_{ij}\) are observed and expected confusion counts, and \(L\) is the number of score levels. QWK ranges from \(-1\) to \(1\) and higher QWK is better: \(1\) indicates perfect agreement, \(0\) indicates chance-level agreement, and \(-1\) indicates complete disagreement.

\subsection{Reflect-and-Revise}
At iteration \(t\), we maintain a candidate pool \(\mathcal{C}_{t-1}\) of size at most \(K\). For each candidate rubric \(\rubric \in \mathcal{C}_{t-1}\), we first score all training samples and collect failed examples:
\begin{equation}
\mathcal{E}(\rubric)=\{(x_i,y_i,\hat{y}_i,z_i)\mid \hat{y}_i \neq y_i\}.
\end{equation}

For each Monte Carlo trial \(m\in\{1,\dots,M\}\) and each batch size \(b \in \mathcal{B}\), we draw a balanced subsample \(\tilde{\mathcal{E}} \subset \mathcal{E}(\rubric)\) of size \(b\), stratified across score levels so that each human-score value is represented as equally as possible. The model then rewrites the rubric by reflecting on the sampled errors and their rationales:
\begin{equation}
\rubric' = \procname{ReviseRubric}(\rubric, \tilde{\mathcal{E}}).
\end{equation}
Concretely, the revision prompt presents the current rubric together with each error case in \(\tilde{\mathcal{E}}\)---including the essay text, the model's predicted score and rationale, and the human score---and asks the model to identify systematic scoring-error patterns and propose targeted rubric modifications (see Appendix~\ref{sec:appendix_prompts} for the full prompt templates).

\subsection{Iterative Update}
Let \(\mathcal{N}_t\) be the union of all newly revised rubrics and the previous top candidates \(\mathcal{C}_{t-1}\). Every rubric in \(\mathcal{N}_t\) is re-evaluated on \(\mathcal{D}_{\mathrm{train}}\), and we retain the top \(K\) by QWK:
\begin{equation}
\mathcal{C}_t=\mathrm{TopK}_{\rubric\in\mathcal{N}_t}\ \mathrm{QWK}(\rubric).
\end{equation}
The global best rubric \(\rubric_{\mathrm{best}}\) is updated only when the best candidate of \(\mathcal{C}_t\) improves over the previous best training QWK. We repeat this process for \(T\) iterations and return \(\rubric_{\mathrm{best}}\).

\section{Experiments}
\label{sec:experiments}

\subsection{Datasets}
We evaluate on three essay scoring benchmarks. The Automated Student Assessment Prize (ASAP) dataset~\citep{ben2012asap} consists of student essays from U.S.\ standardized tests; we use essay set~1 (P1), which contains persuasive essays scored on an integer scale from 1 to 6 by human raters. ASAP 2.0~\citep{Crossley2025ASAP2} is a corpus of source-based argumentative essays written by U.S.\ secondary students across seven prompts, scored on a 1--6 integer scale with consistent rubrics and accompanying source texts; we use the ``Exploring Venus'' subset in our experiments. The TOEFL11 corpus~\citep{blanchard2013toefl} contains English essays written by non-native speakers across eight essay prompts, labeled at three proficiency levels (high, medium, low). For each dataset, we use 100 training samples for rubric refinement and evaluate on 100 held-out test samples. The expert and simplest seed rubrics of ASAP are provided in Appendix~\ref{sec:appendix_rubrics}.

For TOEFL11, the original rubric defines five proficiency levels (scores 1--5), but the dataset labels use three levels (high, medium, low). We adopt the rubric descriptions for score 4 as \textit{high} (mapped to score 3), score 3 as \textit{medium} (mapped to score 2), and score 2 as \textit{low} (mapped to score 1). 

\subsection{Experimental Setup}
We compare our Reflect-and-Revise method against two baselines. The first is the \textbf{Human Rubric} baseline, which directly uses the original human-authored rubric without any refinement. The second is \textbf{AutoCalibrate}, which follows AutoCalibrate~\citep{liu-etal-2024-calibrating} in performing a single-pass rubric revision using score mismatches between model predictions and human labels, without incorporating the model's chain-of-thought rationales. As shown in \autoref{tab:rubric_rewrite_data_requirements}, AutoCalibrate uses the most signals among prior methods (human-labeled scores, evaluated texts, source passages, and model-predicted scores), making it the closest baseline to our approach. We chose AutoCalibrate as our primary baseline also because many rubric refinement methods discussed in \autoref{tab:rubric_rewrite_data_requirements} incorporate post-processing steps beyond rubric modification (e.g., score calibration or aggregation), making it difficult to isolate the effect of rubric refinement itself. To further ensure a fair comparison, we initialize AutoCalibrate from the same human-authored rubric as our method, rather than generating an initial rubric from scratch with an LLM as in the original work.

We evaluate using three frontier LLMs accessed via the OpenRouter API~\citep{openrouter}: \texttt{GPT-5-mini}~\citep{gpt5}, \texttt{Gemini~3~Flash}~\citep{Google2025GeminiFlash}, and \texttt{Qwen3-Next-80B-A3B-Instruct}~\citep{qwen3-next}. 
We report QWK, accuracy, spearman correlation, Macro-F1, and MAE(mean absolute error) between model predictions and human scores on the held-out test set in a zero-shot setting. We experiment with two seed rubric variants. The first is an \textit{expert} rubric, the full human-authored rubric provided with the dataset, which tests whether iterative refinement can further improve an already well-crafted rubric. The second is a \textit{simplest} rubric, a minimal instruction specifying only the score scale (e.g., ``Based on the response's content, rate the response on a scale of 1 to 6.''). This variant serves two purposes: it examines whether our method can reduce the manual effort required for rubric authoring, and it tests whether starting from a minimal seed allows the optimization to explore a broader search space than one constrained by human-designed rubric structures. Detailed hyperparameters, including model-specific generation parameters, are provided in Appendix~\ref{sec:appendix_hparams}.

\section{Experimental Results}
\label{sec:results}

\subsection{Main Results}
\label{sec:main_results}
Results on ASAP, ASAP 2.0 and TOEFL11 are provided in Tables~\ref{tab:asap1},~\ref{tab:asap2} and~\ref{tab:toefl} respectively. For ASAP and ASAP 2.0, our method outperforms both the human rubric and AutoCalibrate baselines across all three models, with QWK gains of up to +0.438 over the human rubric (GPT-5-mini on ASAP) and +0.233 over AutoCalibrate (Qwen3-80B-A3B on ASAP). For TOEFL11, our method also achieves the best performance for two out of three models starting from simplest seed rubrics.

\subsection{Refinement from Simplest Seed Rubric}
\label{sec:simplest_seed}
A key practical question is whether rubric refinement can reduce the burden of manual rubric authoring for LLMs. To investigate this, we apply our method starting from a \textit{simplest} seed rubric and compare the resulting test QWK against the human expert rubric. As shown in \autoref{tab:simplest}, our method achieves comparable or better performance than the human expert rubric in most settings, with QWK gains of up to +0.272 (GPT-5 mini on ASAP) over the human rubric. This suggests that iterative refinement can substantially reduce the need for manual rubric authoring.

\begin{table}[t]
\centering
\small
\resizebox{\columnwidth}{!}{%
\begin{tabular}{llrr}
\toprule
\textbf{Dataset} & \textbf{LLM} & \textbf{Ours from} & \textbf{$\Delta$ vs.\ expert} \\
 & & \textbf{simplest} & \textbf{rubric} \\
\midrule
ASAP & GPT-5 mini & 0.314 & +0.272 \\
 & Gemini 3 Flash & 0.580 & +0.153 \\
 & Qwen3-80B-A3B & 0.154 & +0.034 \\
\midrule
ASAP 2.0 & GPT-5 mini & 0.452 & +0.094 \\
 & Gemini 3 Flash & 0.651 & +0.038 \\
 & Qwen3-80B-A3B & 0.509 & $-$0.066 \\
\midrule
TOEFL11 & GPT-5 mini & 0.394 & $-$0.053 \\
 & Gemini 3 Flash & 0.663 & +0.088 \\
 & Qwen3-80B-A3B & 0.493 & +0.129 \\
\bottomrule
\end{tabular}%
}
\caption{QWK comparison: rubrics refined from the simplest seed (``Based on the response's content, rate the response on a scale of 1 to 6.'') vs.\ the rubric written by human experts. $\Delta$ denotes QWK improvement over the baseline using human expert rubrics. Our method achieves comparable or better performance than the human expert rubric in most settings, demonstrating that iterative refinement can substantially reduce the need for manual rubric authoring.}
\label{tab:simplest}
\end{table}

\section{Analysis}
\label{sec:analysis}

\paragraph{Analysis of Refined Rubrics}
\begin{table*}[t]
\centering
\begin{tabular}{lp{5.5cm}rrr}
\toprule
Pattern & Example Snippet & Before & After & $\Delta$ \\
\midrule
Conditional Gating & \textit{…If it explores the "how" and "why" behind…} & 0.0 & 21.2 & +21.2 \\
Quantitative Threshold & \textit{…elaboration is limited to 2-3 sentences per point, it should …} & 0.0 & 9.1 & +9.1 \\
Concrete Exemplification & \textit{…if minor factual inaccuracies exist (e.g., misspelled names like…} & 0.3 & 7.0 & +6.7 \\
Score Cap / Demotion & \textit{…Do not award a 5 if the essay’s analysis is…} & 0.0 & 2.7 & +2.7 \\
Boundary / Tie-Break & \textit{…- A 5 vs. 6 Distinction: A 6 must explore broader…} & 0.0 & 2.6 & +2.6 \\
Stepwise Workflow & \textit{…scoring algorithm (must be followed in order) Step 1…} & 0.0 & 2.6 & +2.6 \\
\bottomrule
\end{tabular}
\caption{Regex-based match counts in human-authored rubrics (\textbf{Before}) vs.\ iteratively refined rubrics (\textbf{After}), averaged over 9 runs. Each pattern is detected by case-insensitive keyword matching. \textbf{Conditional Gating} (\textit{if, when, unless, provided that}); \textbf{Quantitative Threshold} (\textit{at least, at most, <=, >=, N reasons/examples/sentences, N\%}); \textbf{Concrete Exemplification} (\textit{e.g., for example, for instance}); \textbf{Score Cap / Demotion} (\textit{cannot be Score, must not receive, do not award, downgrade, demotion}); \textbf{Boundary / Tie-Break} (\textit{tie-break, borderline, threshold, N vs N, between adjacent}); \textbf{Stepwise Workflow} (\textit{step N, checklist, workflow, procedure, in order}). $\Delta$ = After $-$ Before. Detailed definitions and examples of these patterns are provided in \autoref{fig:pattern_explanation} in Appendix~\ref{app:rubric_analysis}.}
\label{tab:rubric_pattern_changes}

\end{table*}
\label{sec:qualitative}
To understand how refinement changes rubric content, we analyze the refined rubrics compared to the original human expert rubrics. \autoref{tab:rubric_comparison} in Appendix \ref{app:rubric_analysis} presents a word-count-based comparison of the original and refined rubrics. The refined rubrics tend to be longer up to approximately 10 times the length of the original rubrics (GPT-5 mini on TOEFL11). Overlapping words ratio between the original and refined rubrics are about 20\% on average, indicating that the refined rubrics often introduce substantial new content. Especially for GPT-5 mini, overlapping words ratio is as low as 5.1\% on ASAP and leads to a large QWK gain of +0.438 suggesting that the refinement process can discover rubric formulations that are substantially different from human-authored rubrics and better suited for the model's reasoning and scoring process. On the other hand, GPT-5 mini's refined rubric on TOEFL11 has a low overlapping words ratio of 3.3\% but does not improve over the human rubric, suggesting that not all substantial modifications lead to performance gains and have potentials for overfitting to the training set. 

We investigate what kinds of content the refinement process adds to rubrics. Through manual inspection of a sample of refined rubrics, we identified recurring linguistic and structural motifs and grouped them into six categories of rubric patterns: \textbf{Conditional Gating}, \textbf{Quantitative Threshold}, \textbf{Concrete Exemplification}, \textbf{Score Cap / Demotion}, \textbf{Boundary / Tie-Break}, and \textbf{Stepwise Workflow}. We then design regex-based detectors for each pattern and apply them to the original human rubrics and the refined rubrics, counting the number of matches for each pattern. The results are shown in \autoref{tab:rubric_pattern_changes}. Detailed definitions and examples of these patterns are provided in \autoref{fig:pattern_explanation} in Appendix~\ref{app:rubric_analysis}.

The original human-authored rubrics contain almost none of these patterns. After refinement, all categories show substantial increases. Conditional Gating exhibits the largest gain (+21.2), indicating that the refinement process frequently introduces explicit conditional rules to guide scoring decisions. Quantitative Threshold patterns (+9.1) show that the refined rubrics often specify numeric cutoffs to make scoring criteria more precise. Concrete Exemplification patterns suggest that the refined rubrics provide more specific examples to illustrate abstract criteria which is often missing in human rubrics. 
\autoref{fig:rubric_pattern_bg_asap_1_google_gemini_3_flash_preview_base_expert_True_train100_iteration5_top3_bs4_8_12_mc4} in Appendix~\ref{app:rubric_analysis} provides an example of a full refined rubric with detected patterns highlighted.


\paragraph{Iterative Refinement and Monte Carlo Effects}
To analyze the effect of iterative refinement and Monte Carlo trials, which controls depth and breadth of exploration, respectively, we track the best QWK in training dataset at each iteration. Figure~\ref{fig:training_qwk} shows these trajectories of Gemini 3 Flash on ASAP 2.0. By using large number of Monte Carlo trials, our method explores a wide variety of rubric modifications at each iteration, which leads to steady improvements in training QWK across iterations for all three models. 

About gains from iteration, we calculate the stepwise improvement in QWK at each iteration, defined as the difference in best training QWK between steps. Figure~\ref{fig:qwk_gains} plots these stepwise improvements across iterations for all three datasets averaging across models. The largest gains tend to occur in early iterations, with diminishing returns in later iterations.

\begin{figure}[t]
\centering
\includegraphics[width=0.9\linewidth]{figures/fig_iterative_refinement_mc_ASAP2.pdf}
\caption{Best training QWK across iterations of Gemini 3 Flash on ASAP 2.0. MC stands for Monte Carlo trials. By using a large number of Monte Carlo trials, our method explores a wide variety of rubric modifications at each iteration, which leads to improvements in training QWK across iterations.}
\label{fig:training_qwk}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.9\linewidth]{figures/fig_stepwise_improvement.pdf}
\caption{Best training QWK improvement between refinement steps, averaged across models for each dataset. $s_{t-n}\!\to\!s_{t}$ denotes the relative improvement from step $t-n$ to step $t$. The largest gains tend to occur in early iterations.}
\label{fig:qwk_gains}
\end{figure}

\paragraph{Ablation on Iterative and Rationale Components}
To quantify the contribution of iterative refinement and chain-of-thought rationales, we perform an ablation study using ASAP 2.0 dataset. We compare our full method against two ablated variants: (1) \textbf{w/o Iteration}, which performs only a single revision step (i.e., \(T=1\)) without further iterations, and (2) \textbf{w/o Rationale}, which performs iterative refinement but omits the model's chain-of-thought rationales from the revision prompt, relying solely on score mismatches as in AutoCalibrate. Table~\ref{tab:ablation} reports the QWK results for these variants. The full method outperforms both ablated variants across all three models, confirming that both iterative refinement and chain-of-thought rationales contribute to performance improvements.

\begin{table}[t]
\centering
\small
\begin{tabular}{llr}
\toprule
LLM & Method & QWK \\
\midrule
Gemini 3 Flash & Ours & \textbf{0.700} \\
Gemini 3 Flash & w/o Iteration ($T=1$) & 0.616 \\
Gemini 3 Flash & w/o Rationale & \underline{0.665} \\
\midrule
GPT-5 mini & Ours & \textbf{0.537} \\
GPT-5 mini & w/o Iteration ($T=1$) & 0.325 \\
GPT-5 mini & w/o Rationale & \underline{0.527} \\
\midrule
Qwen3-80B-A3B & Ours & \textbf{0.636} \\
Qwen3-80B-A3B & w/o Iteration ($T=1$) & 0.522 \\
Qwen3-80B-A3B & w/o Rationale & \underline{0.561} \\
\bottomrule
\end{tabular}
\caption{The full method (Ours) outperforms both ablated variants, confirming the importance of both iterative refinement and chain-of-thought rationales for maximizing rubric effectiveness.}
\label{tab:ablation}
\end{table}

\paragraph{Scoring Distribution Analysis}
To visualize how rubric refinement changes scoring behavior, \autoref{fig:confusion} in Appendix~\ref{app:confusion} shows confusion matrices for Qwen3-80B-A3B with the human expert rubric versus our refined rubric across all three datasets. After refinement, the model's predicted scores show higher agreement with human labels, as indicated by stronger diagonal patterns in the confusion matrices. 

\paragraph{Ablation on Training Data Size}
\begin{table}[t]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{llrrrr}
\toprule
\textbf{Dataset} & \textbf{LLM} & Baseline & \textbf{$N=20$} & \textbf{$N=50$} & \textbf{$N=100$} \\
\midrule
ASAP & Gemini 3 Flash & 0.427 & 0.267 & \underline{0.538} & \textbf{0.646} \\
 & GPT-5 mini & 0.042 & 0.249 & \underline{0.409} & \textbf{0.480} \\
 & Qwen3-80B-A3B & 0.121 & \underline{0.395} & 0.383 & \textbf{0.473} \\
\midrule
ASAP 2.0 & Gemini 3 Flash & 0.613 & \textbf{0.724} & 0.619 & \underline{0.700} \\
 & GPT-5 mini & 0.358 & \textbf{0.544} & \underline{0.540} & 0.537 \\
 & Qwen3-80B-A3B & 0.575 & \underline{0.616} & 0.557 & \textbf{0.636} \\
\bottomrule
\end{tabular}%
}
\caption{Effect of training sample size ($N$) on test QWK. Baseline uses the human expert rubric without optimization. Bold indicates the best and underline indicates the second-best QWK among training sizes for each model.}
\label{tab:train_size}
\end{table}
To analyze the effect of training data size on rubric refinement, we perform an ablation study using ASAP and ASAP 2.0 datasets. As shown in \autoref{tab:train_size}, we experiment with training data sizes of 20, 50, and 100 samples. Our method shows comparable or better performance than the baseline even with as few as 20 training samples.

% \paragraph{Cross-model rubric transferability}
% An important question for practical deployment is whether rubrics optimized for one model can benefit other models. To investigate this, we evaluate rubrics refined by each model on the other two models. Table~\ref{tab:cross_model} shows cross-model QWK for expert-seed rubrics refined with our method.

% \begin{table*}[t]
% \centering
% \small
% \begin{tabular}{lrrr}
% \toprule
% \textbf{Source $\backslash$ Evaluator} & \textbf{GPT-5 mini} & \textbf{Gemini 3 Flash} & \textbf{Qwen3-80B-A3B} \\
% \multicolumn{4}{l}{\textit{ASAP P1}} \\
% Human & 0.042 & 0.427 & 0.121 \\
% GPT-5 mini & \textbf{\cellcolor{gray!20}0.429} & 0.424 & \textbf{0.369} \\
% Gemini 3 Flash & 0.235 & \textbf{\cellcolor{gray!20}0.643} & 0.355 \\
% Qwen3-80B-A3B & 0.060 & 0.297 & \cellcolor{gray!20}0.339 \\
% \midrule
% \multicolumn{4}{l}{\textit{ASAP 2.0}} \\
% Human & 0.358 & 0.613 & \textbf{0.575} \\
% GPT-5 mini & \cellcolor{gray!20}0.398 & \textbf{0.651} & 0.329 \\
% Gemini 3 Flash & \textbf{0.399} & \cellcolor{gray!20}0.639 & 0.312 \\
% Qwen3-80B-A3B & 0.321 & 0.644 & \cellcolor{gray!20}0.558 \\
% \midrule
% \multicolumn{4}{l}{\textit{TOEFL11}} \\
% Human & 0.447 & \textbf{0.574} & 0.364 \\
% GPT-5 mini & \textbf{\cellcolor{gray!20}0.485} & 0.557 & 0.341 \\
% Gemini 3 Flash & 0.360 & \cellcolor{gray!20}0.477 & 0.483 \\
% Qwen3-80B-A3B & 0.107 & 0.376 & \textbf{\cellcolor{gray!20}0.499} \\
% \bottomrule
% \end{tabular}
% \caption{Cross-model rubric transferability (QWK). Each cell shows QWK when the rubric optimized by the \textit{source} model (row) is used by the \textit{evaluator} model (column). Shaded diagonal cells denote same-model results. ``Human'' row shows the unrefined human rubric baseline. Bold values indicate the best QWK per evaluator model within each dataset.}
% \label{tab:cross_model}
% \end{table*}

% The results reveal that transferability is highly asymmetric and dataset-dependent. On ASAP P1, the rubric refined by Gemini 3 Flash yields the best QWK not only for Gemini itself (0.576) but also for Qwen3-80B-A3B (0.524), substantially outperforming Qwen's own refined rubric (0.213). Similarly, on ASAP 2.0, the rubric refined by GPT-5 mini achieves the highest QWK when used by Gemini 3 Flash (0.688), surpassing Gemini's own rubric (0.639). These cases suggest that some models produce rubrics that capture more generalizable scoring criteria. However, the reverse transfers are often ineffective: on ASAP P1, the Qwen-refined rubric yields near-zero QWK for GPT-5 mini (0.001). On TOEFL11, the unrefined human rubric remains the strongest option for GPT-5 mini and Gemini 3 Flash, indicating that refined rubrics can overfit to model-specific scoring patterns on certain datasets. Overall, while cross-model transfer can sometimes be beneficial, rubrics are generally most effective when optimized for the target model.

\section{Conclusion}
We proposed Reflect-and-Revise, an iterative framework that refines scoring rubrics for LLM-based Automated Essay Scoring by prompting models to reflect on their own chain-of-thought rationales and score discrepancies with human labels. Experiments on three benchmarks (ASAP, ASAP 2.0, and TOEFL11) with three LLMs demonstrated that our method consistently improves scoring agreement with human raters, achieving QWK gains of up to +0.438 over human-authored rubrics. Starting from a minimal seed rubric that specifies only the score scale, our method matched or exceeded expert rubric performance in seven out of nine dataset-model combinations, suggesting that iterative refinement can substantially reduce the manual effort of rubric authoring for LLMs. Ablation studies confirmed that both iterative refinement and chain-of-thought rationales contribute to the observed improvements. Analysis of the refined rubrics revealed that the refinement process introduces explicit procedural structures, such as conditional gating rules and quantitative thresholds, that are absent from human-authored rubrics, highlighting a gap between rubrics designed for human raters and those effective for LLMs.

\section*{Limitations}
Our work has several limitations. First, our quantitative analysis of refined rubric content relies on regex-based pattern matching, whose results depend on the predefined pattern definitions and their matching accuracy. More sophisticated analysis methods could provide deeper insights into the characteristics of effective rubrics. Second, our method requires a moderate number of human-labeled training samples (100 in our experiments) for rubric refinement. Although we showed that comparable gains are achievable with as few as 20 samples, the method still depends on the availability of human-scored data. Third, iterative refinement incurs substantial computational cost due to repeated LLM inference over multiple iterations and Monte Carlo trials; for instance, a single optimization run on ASAP 2.0 with Gemini 3 Flash cost approximately \$170 in API calls. Finally, our evaluation is limited to English essay scoring across three benchmarks. The generalizability of our method to other languages, writing genres, and evaluation tasks remains to be investigated.

\bibliography{custom}
\appendix

\section{Algorithm}
\label{sec:appendix}

Algorithm~\ref{alg:rubric_refinement} gives the full pseudocode for iterative rubric refinement.

\begin{algorithm*}[t]
\caption{Iterative Rubric Refinement}
\label{alg:rubric_refinement}
\begin{algorithmic}[1]
\Require Training set $\mathcal{D}_{\mathrm{train}} = \{(x_i, y_i)\}_{i=1}^{N}$, seed rubric $\rubric_0$, iterations $T$, pool size $K$, Monte Carlo trials $M$, batch sizes $\mathcal{B}$, evaluator LLM
\Ensure Best rubric $\rubric_{\mathrm{best}}$
\State $\mathcal{C}_0 \gets \{\rubric_0\}$;\; $\rubric_{\mathrm{best}} \gets \rubric_0$;\; $q_{\mathrm{best}} \gets \mathrm{QWK}(\rubric_0)$
\For{$t = 1$ \textbf{to} $T$}
    \State $\mathcal{N}_t \gets \emptyset$
    \For{\textbf{each} $\rubric \in \mathcal{C}_{t-1}$}
        \State Score all $(x_i,y_i) \in \mathcal{D}_{\mathrm{train}}$ with $\rubric$ $\to$ $(\hat{y}_i, z_i)$
        \State $\mathcal{E}(\rubric) \gets \{(x_i,y_i,\hat{y}_i,z_i) \mid \hat{y}_i \neq y_i\}$
        \For{$m = 1$ \textbf{to} $M$}
            \For{\textbf{each} $b \in \mathcal{B}$}
                \State $\tilde{\mathcal{E}} \gets \procname{BalancedSample}(\mathcal{E}(\rubric),\, b)$
                % \Comment{Stratified by score level}
                \State $\rubric' \gets \procname{ReviseRubric}(\rubric,\, \tilde{\mathcal{E}})$
                % \Comment{LLM revises rubric}
                \State $\mathcal{N}_t \gets \mathcal{N}_t \cup \{\rubric'\}$
            \EndFor
        \EndFor
    \EndFor
    \State $\mathcal{N}_t \gets \mathcal{N}_t \cup \mathcal{C}_{t-1}$
    % \Comment{Retain previous top candidates}
    \State Re-evaluate all $\rubric \in \mathcal{N}_t$ on $\mathcal{D}_{\mathrm{train}}$
    \State $\mathcal{C}_t \gets \mathrm{TopK}_{\rubric \in \mathcal{N}_t}\;\mathrm{QWK}(\rubric)$
    \If{$\max_{\rubric \in \mathcal{C}_t} \mathrm{QWK}(\rubric) > q_{\mathrm{best}}$}
        \State $\rubric_{\mathrm{best}} \gets \arg\max_{\rubric \in \mathcal{C}_t} \mathrm{QWK}(\rubric)$;\; $q_{\mathrm{best}} \gets \mathrm{QWK}(\rubric_{\mathrm{best}})$
    \EndIf
\EndFor
\State \Return $\rubric_{\mathrm{best}}$
\end{algorithmic}
\end{algorithm*}

\procname{BalancedSample}$(\mathcal{E}, b)$ partitions the error set $\mathcal{E}$ into buckets by human-score level, then draws $\lfloor b / L' \rfloor$ examples from each bucket (where $L'$ is the number of non-empty buckets), ensuring that each score level is represented as equally as possible.

\section{Prompt Templates}
\label{sec:appendix_prompts}

This section lists the exact prompt templates used in our pipeline. Placeholders enclosed in braces (e.g., \texttt{\{rubric\}}) are filled at runtime.

\subsection{Evaluation Prompt}
\label{sec:eval_prompt}
The following prompt is used to score each essay with the current rubric. The LLM is instructed to output a rationale followed by an integer score.

\begin{tcolorbox}[colback=white, colframe=RoyalBlue, title=Evaluation Prompt, fonttitle=\bfseries\small, fontupper=\small, breakable]
You are an expert rater for a high-stakes English writing exam for second-language learners.
Evaluate the response strictly using the scoring guideline. Choose exactly one score from the scoring guideline's score points.

\texttt{\# Essay Prompt}\\
\texttt{"""\{essay\_prompt\}"""}

\texttt{\# Response}\\
\texttt{"""\{response\}"""}

\texttt{\# Scoring Guideline}\\
\texttt{"""\{rubric\}"""}

\texttt{\# Output format (follow exactly)}\\
\texttt{Rationale: [<<<Brief evidence-based rationale.>>>]}\\
\texttt{Rating: [<<<One integer score only.>>>]}
\end{tcolorbox}

\subsection{Error-Case Format for Rubric Revision}
\label{sec:error_format}
For each incorrectly scored example fed into the revision prompt, the following format is used to present the error case to the model. When rationales are included (\texttt{with\_rationale=True}), the model's chain-of-thought is shown alongside its predicted score.

\begin{tcolorbox}[colback=white, colframe=RoyalBlue, title=Error-Case Format (with rationale), fonttitle=\bfseries\small, fontupper=\small, breakable]
\texttt{Assistant input:}\\
\texttt{Essay prompt:}\\
\texttt{"""\{essay\_prompt\}"""}\\
\texttt{Essay to be rated:}\\
\texttt{"""\{response\}"""}\\
\texttt{Assistant rationale:}\\
\texttt{"""\{rationale\}"""}\\
\texttt{Assistant score:}\\
\texttt{"""\{rating\}"""}\\
\texttt{Desired score:}\\
\texttt{"""\{desired\_rating\}"""}
\end{tcolorbox}

\subsection{Rubric Revision Prompt}
\label{sec:revision_prompt}
The revision prompt wraps the current rubric and the sampled error cases, and instructs the model to output a revised rubric.

\begin{tcolorbox}[colback=white, colframe=RoyalBlue, title=Rubric Revision Prompt (with rationale), fonttitle=\bfseries\small, fontupper=\small, breakable]
I asked an assistant to grade essays using the scoring guideline below:

\texttt{```}\\
\texttt{\{current\_rubric\}}\\
\texttt{```}

Here are grading examples that include the assistant input, the assistant rationale, the assistant score, and the desired score:

\texttt{```}\\
\texttt{\{examples\}}\\
\texttt{```}

Revise the scoring guideline to improve score agreement so the assistant's future ratings align more closely with the desired scores.

Requirements:\\
1. Use the rationale patterns to identify why the assistant over-scored or under-scored, and improve the scoring guideline guidance accordingly to reduce score mismatches.

Output rules:\\
-- Return only the revised scoring guideline.\\
-- Use exactly one fenced code block with triple backticks.\\
-- Do not include any text before or after the code block.
\end{tcolorbox}

When rationales are not used (\texttt{with\_rationale=False}), the error-case format omits the \texttt{Assistant rationale} field, and the requirement in the revision prompt is replaced with: ``Use score mismatch patterns to identify where the scoring guideline guidance is insufficient or ambiguous, and revise it to reduce score mismatches.''

\section{Hyperparameters}
\label{sec:appendix_hparams}

Table~\ref{tab:hyperparams} summarizes the optimization hyperparameters. We adopt the number of Monte Carlo trials ($M{=}4$) from AutoCalibrate. For error batch sizes, AutoCalibrate originally used $\mathcal{B}{=}\{1,2,4\}$, but considering the improved capacity of recent LLMs to process longer contexts, we increase them to $\mathcal{B}{=}\{4,8,12\}$. For the number of iterations, the original self-refinement loop in \citet{madaan2023selfrefine} used $T{=}4$; we set $T{=}5$, expecting that the stronger reasoning and instruction-following capabilities of recent models would allow productive refinement over additional cycles. The AutoCalibrate baseline uses $T{=}1$ (single-pass revision) without rationales, while our Reflect-and-Revise method performs iterative refinement with rationale-based feedback. Both methods maintain $K{=}3$ top candidates and use $N{=}100$ training samples.

\begin{table}[h]
  \centering
  \small
  \begin{tabular}{lcc}
    \toprule
    \textbf{Hyperparameter} & \textbf{Ours} & \textbf{AutoCalibrate} \\
    \midrule
    Iterations ($T$) & 5 & 1 \\
    Top-$K$ candidates & 3 & 3 \\
    Monte Carlo trials ($M$) & 4 & 4 \\
    Batch sizes ($\mathcal{B}$) & \{4, 8, 12\} & \{4, 8, 12\} \\
    Training samples ($N$) & 100 & 100 \\
    With rationale & Yes & No \\
    \bottomrule
  \end{tabular}
  \caption{Optimization hyperparameters for rubric refinement.}
  \label{tab:hyperparams}
\end{table}

For all models, we set the maximum output tokens to 8192. For GPT-5-mini and Gemini 3 Flash, we set reasoning effort to low. All other generation parameters were left at their default values for each model.

Trying our method on ASAP 2.0 with Gemini 3 Flash cost approximately \$170 (28 million input tokens and 50 million output tokens) in API calls, while GPT-5 mini and Qwen3-80B-A3B cost approximately \$50 and \$13, respectively. The exact costs depend on the number of tokens generated by the model, which can vary based on the length of the essays and the complexity of the revised rubrics.


\section{Seed Rubrics}
\label{sec:appendix_rubrics}

This section lists the expert (human-authored) and simplest seed rubrics of the ASAP dataset used in our experiments. The expert rubric is the original human-authored rubric provided with the dataset, while the simplest rubric is a minimal description specifying only the score scale.

\subsection{Expert rubric}

\begin{tcolorbox}[colback=white, colframe=gray, fontupper=\small, breakable]
Score Point 1: An undeveloped response that may take a position but offers no more than very minimal support. Typical elements:
- Contains few or vague details.
- Is awkward and fragmented.
- May be difficult to read and understand.
- May show no awareness of audience.

Score Point 2: An under-developed response that may or may not take a position. Typical elements:
- Contains only general reasons with unelaborated and/or list-like details.
- Shows little or no evidence of organization.
- May be awkward and confused or simplistic.
- May show little awareness of audience.

Score Point 3: A minimally-developed response that may take a position, but with inadequate support and details. Typical elements:
- Has reasons with minimal elaboration and more general than specific details.
- Shows some organization.
- May be awkward in parts with few transitions.
- Shows some awareness of audience.

Score Point 4: A somewhat-developed response that takes a position and provides adequate support. Typical elements:
- Has adequately elaborated reasons with a mix of general and specific details.
- Shows satisfactory organization.
- May be somewhat fluent with some transitional language.
- Shows adequate awareness of audience.

Score Point 5: A developed response that takes a clear position and provides reasonably persuasive support. Typical elements:
- Has moderately well elaborated reasons with mostly specific details.
- Exhibits generally strong organization.
- May be moderately fluent with transitional language throughout.
- May show a consistent awareness of audience.

Score Point 6: A well-developed response that takes a clear and thoughtful position and provides persuasive support. Typical elements:
- Has fully elaborated reasons with specific details.
- Exhibits strong organization.
- Is fluent and uses sophisticated transitional language.
- May show a heightened awareness of audience.

Note: 
I have made an effort to remove personally identifying information from the essays using the Named Entity Recognizer (NER). The relevant entities are identified in the text and then replaced with a string such as "PERSON", "ORGANIZATION", "LOCATION", "DATE", "TIME", "MONEY", "PERCENT", "CAPS" (any capitalized word) and "NUM" (any digits). Please do not penalize the essay because of the anonymizations.
\end{tcolorbox}

\subsection{Simplest rubric}

\begin{tcolorbox}[colback=white, colframe=gray, fontupper=\small]
Based on the response's content, rate the response on a scale of 1 to 6.
\end{tcolorbox}

\section{Analysis of Rubric Changes}
\label{app:rubric_analysis}
This section provides analyses of the rubric changes introduced by our refinement method. \autoref{tab:rubric_comparison} compares word counts and lexical overlap between the seed and refined rubrics for each dataset-model pair. \autoref{fig:pattern_explanation} defines the six structural patterns used in our regex-based rubric analysis, with representative matched snippets from refined rubrics. \autoref{fig:rubric_pattern_bg_asap_1_google_gemini_3_flash_preview_base_expert_True_train100_iteration5_top3_bs4_8_12_mc4} shows a full refined rubric with detected patterns highlighted.

\input{figures/tab_rubric_comparison}
\input{figures/fig_pattern_explanation}


\input{figures/rubric_pattern_highlights_bg/fig_rubric_with_pattern_highlight_asap_1_google_gemini_3_flash_preview_base_expert_True_train100_iteration5_top3_bs4_8_12_mc4.tex}

\section{Confusion Matrices}
\label{app:confusion}
\autoref{fig:confusion} presents confusion matrices comparing the scoring behavior of Qwen3-80B-A3B under the human expert rubric and our refined rubric across all three datasets. Each matrix is row-normalized to show the distribution of predicted scores for each gold label; raw counts are overlaid. After refinement, predictions become more concentrated along the diagonal, reflecting improved alignment with human annotations and reduced off-diagonal errors.

\begin{figure*}[t]
\centering
\includegraphics[width=0.75\textwidth]{figures/confusion_matrices.pdf}
\caption{Confusion matrices for Qwen3-80B-A3B with the human expert rubric (top) and our refined rubric (bottom) across three datasets. Cell colors indicate row-normalized proportions; numbers show raw counts. Our refined rubric produces predictions more concentrated along the diagonal, indicating better agreement with human annotations.}
\label{fig:confusion}
\end{figure*}
\end{document}
