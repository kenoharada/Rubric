\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[review]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{colortbl}
\usepackage[most]{tcolorbox}
\definecolor{RoyalBlue}{RGB}{65,105,225}
\colorlet{White}{white}
\newcommand{\procname}[1]{\textsc{#1}}
\newcommand{\rubric}{\mathrm{rubric}}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Automated Refinement of Essay Scoring Rubrics \\for Language Models via Reflect-and-Revise}

\author{
 \textbf{Keno Harada},
 \textbf{Lui Yoshida},
 \textbf{Takeshi Kojima},
 \textbf{Yusuke Iwasawa},
 \textbf{Yutaka Matsuo}
\\
The University of Tokyo
\\
 \small{
 \texttt{{keno.harada@weblab.t.u-tokyo.ac.jp}}}
 }

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\begin{abstract}
Large Language Models (LLMs) are increasingly used for Automated Essay Scoring (AES), yet the scoring rubrics they rely on are typically designed for human raters and may not be optimal for LLMs. Inspired by the calibration process that human raters undergo before formal scoring, we propose Reflect-and-Revise, an iterative framework that refines scoring rubrics by prompting models to reflect on their own chain-of-thought rationales and observed score discrepancies with human labels. At each iteration, the model identifies systematic scoring error patterns from sampled mismatches and revises the rubric accordingly. Experiments on three essay scoring benchmarks (ASAP, ASAP 2.0, and TOEFL11) with three LLMs (GPT-5 mini, Gemini 3 Flash, and Qwen3-80B-A3B) show that our method yields substantial improvements in Quadratic Weighted Kappa (QWK), with gains of up to +0.388 over human-authored rubrics on ASAP. Furthermore, starting from a minimal seed rubric that specifies only the score scale, our method can match or exceed the performance of carefully authored expert rubrics in most settings. However, the effectiveness of refinement varies across datasets, with limited gains on TOEFL11 for some models. Our findings demonstrate the potential of iterative rubric refinement for improving LLM-based AES while reducing the burden of manual rubric authoring.\footnote{Optimization and evaluation codes are available at \url{https://anonymous.4open.science/r/ARR2025-1CA3}}
\end{abstract}

\section{Introduction}
Automated Essay Scoring (AES) systems powered by Large Language Models (LLMs) are increasingly expected to provide real-time, scalable feedback for students and alleviate the grading burden on instructors~\citep{mizumoto2023aes,yancey-etal-2023-rating,naismith-etal-2023-automated,pack2024validity}. Typically, these systems employ static, pre-defined rubrics to guide the evaluation. However, it remains an open question whether rubrics designed for human raters are optimal for LLMs. When human raters use a rubric, they often engage in a collaborative calibration process: they score sample essays, discuss discrepancies in their judgments, and refine their shared understanding of the criteria to ensure consistency~\citep{trace2016rubricsnegotiate,ozfidan2022rubricdev,yoo-etal-2025-dress}. This iterative, reflective practice is overlooked in current LLM-based AES, potentially limiting their alignment with human scoring patterns.

Recent studies show that LLMs have the ability to refine their own outputs especially when there is reliable external feedback~\citep{madaan2023selfrefine,kamoi2024selfcorrect}. Prompt optimization techniques leverage these capabilities to update prompts to maximize a targeted metric and show performance improvements in various tasks such as multi-hop reasoning, instruction following and privacy-aware delegation~\citep{khattab2023dspy,opsahl-ong-etal-2024-optimizing,agrawal2025gepa}.

Inspired by these developments and the calibration process of human raters, we propose an iterative Reflect-and-Revise approach for refining rubrics in LLM-based AES.
Specifically, given a small set of 100 sample essays with human scores, the model iteratively refines the rubric by reflecting on its own scoring rationales and the discrepancies between its predicted scores and human labels, with the objective of maximizing Quadratic Weighted Kappa (QWK) between model and human scores.

We evaluate our method on three datasets (ASAP, ASAP 2.0, and TOEFL11) with three models (\texttt{GPT-5 mini}, \texttt{Qwen3-80B-A3B}, and \texttt{Gemini 3 Flash}). Our main findings are as follows:
\begin{itemize}
    \item Iterative rubric refinement with chain-of-thought rationales yields substantial QWK improvements, with gains of up to +0.388 on ASAP P1, consistently outperforming single-pass revision without rationales.
    \item Even starting from a minimal seed rubric that specifies only the score scale, our method achieves comparable or better performance than carefully authored expert rubrics in most settings, substantially reducing the need for manual rubric authoring.
    \item Qualitative analysis reveals that the refinement process introduces explicit decision rules, concrete scoring heuristics, and boundary-case guidance that are absent from the original human-authored rubrics.
    \item However, the effectiveness of refinement varies across datasets, with limited or negative gains on TOEFL11 for some models, indicating room for improvement in cross-dataset robustness.
\end{itemize}

\begin{figure*}[t]
\centering
\includegraphics[width=0.75\textwidth]{figures/confusion_matrices.pdf}
\caption{Confusion matrices for Qwen3-80B-A3B with the human expert rubric (top) and our refined rubric (bottom) across three datasets. Cell colors indicate row-normalized proportions; numbers show raw counts. Our refined rubric produces predictions more concentrated along the diagonal, indicating better agreement with human annotations.}
\label{fig:confusion}
\end{figure*}

\begin{table*}[t]
  \centering
  \resizebox{\textwidth}{!}{%
  \begin{tabular}{lcccccc}
    \toprule
    \textbf{Study} &
    \textbf{Scores} &
    \textbf{Texts} &
    \textbf{Sources} &
    \textbf{Predictions} &
    \textbf{Rationales} &
    \textbf{Iterative} \\
    \midrule
    HD-Eval (\citet{liu-etal-2024-hd}) & $\checkmark$ &  &  &  &  &  $\checkmark$\\
    MTS (\citet{lee-etal-2024-unleashing}) &  &  & $\checkmark$ &  &  &  \\
    \citet{xie2024gradelikehuman} & $\checkmark$ & $\checkmark$ &  &  &  &  \\
    ActiveCritic (\citet{xu2025activecritics}) & $\checkmark$ & $\checkmark$ & $\checkmark$ &  &  &  \\
    AutoCalibrate (\citet{liu-etal-2024-calibrating}) & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$ &  &  \\
    \midrule
    \textbf{Ours} &
    \textbf{$\checkmark$} & \textbf{$\checkmark$} & \textbf{$\checkmark$} & \textbf{$\checkmark$} & \textbf{$\checkmark$} & \textbf{$\checkmark$} \\
    \bottomrule
  \end{tabular}}
  \caption{Comparison of signals and strategies used to refine evaluation rubrics. \textbf{Scores}: human-labeled scores. \textbf{Texts}: texts being evaluated (essays/answers/responses that receive scores). \textbf{Sources}: source passages used to compose responses (e.g., source documents, provided essay themes). \textbf{Predictions}: model-predicted scores. \textbf{Rationales}: model-generated justifications accompanying predicted scores. \textbf{Iterative}: whether the method iteratively refines rubrics over multiple rounds. Our method uniquely leverages all five signals and performs iterative refinement.}
\label{tab:rubric_rewrite_data_requirements}
\end{table*}



\section{Related Work}
For non-verifiable tasks, where judging success is not as straightforward as in math or code, recent research has focused on LLM-based automatic evaluation using checklists and rubrics in prompts~\citep{min-etal-2023-factscore,qin-etal-2024-infobench,lin2024wildbench,wu-etal-2025-lifbench,cook2025ticking,huang2025rubricanchors,gunjal2025rubricsasrewards,viswanathan2025checklistsbetterrewardmodels,lee2025checkeval,xu2025activecritics,liu-etal-2024-calibrating,liu-etal-2024-hd,wen-etal-2025-hpss}. AES is an example of such a non-verifiable task, and various techniques have been proposed~\citep{mizumoto2023aes,xie2024gradelikehuman,lee-etal-2024-unleashing}.

\subsection{Rubric Design for LLM Evaluation}
Recent studies suggest that the relationship between rubric design and LLM evaluation quality is not straightforward. \citet{yoshida2025rubrics} found that making rubrics more detailed does not always lead to performance gains in AES: three out of four models maintained similar scoring accuracy with a simplified rubric, and one model even showed decreased performance with more detailed rubrics. Similarly, \citet{furuhashi2025checklist} identified ``negative items,'' rubric components that are valid for human evaluators but do not improve LLM performance, and showed that removing such items can even boost accuracy. These findings suggest that there remains room to find rubric formulations better suited for LLMs.

\subsection{LLM-based Rubric Refinement}
A growing body of work explores methods for generating or refining evaluation rubrics to improve agreement with human scores~\citep{liu-etal-2024-hd,lee-etal-2024-unleashing,xie2024gradelikehuman,xu2025activecritics,liu-etal-2024-calibrating}. Some methods generate rubrics in a single pass without subsequent revision: for example, generating rubrics from source passages~\citep{lee-etal-2024-unleashing}, from few input--score examples~\citep{xu2025activecritics}, or by rewriting existing rubrics using human-labeled scores and evaluated texts~\citep{xie2024gradelikehuman}. 

Among these, \citet{liu-etal-2024-calibrating} proposed the pipeline closest to ours. Their method samples candidate evaluation criteria from human-scored data, scores a held-out set, and refines the best-performing criteria by analyzing error cases where model scores diverge from human labels. However, their approach differs from ours in two key respects: (1) the refinement loop is executed only once rather than iteratively, and (2) the refinement step does not incorporate the model's chain-of-thought rationales, limiting the diagnostic signal available for rubric revision.

Our method extends this line of work by enabling the LLM to reflect on its own scoring output, including its chain-of-thought rationales, to iteratively refine the rubric. By feeding back not only human-labeled and model-predicted scores but also the model's justifications for those predictions, our approach provides richer diagnostic information for identifying why the current rubric leads to scoring errors. This process effectively mimics the calibration sessions of human evaluators, who refine their interpretations and build shared understanding before formal scoring~\citep{trace2016rubricsnegotiate,ozfidan2022rubricdev,ouyang2022instructgpt,yoo-etal-2025-dress}. Table~\ref{tab:rubric_rewrite_data_requirements} summarizes the signals used for rubric refinement across prior work and our method.


\section{Iterative Rubric Refinement}
\label{sec:method}
Our method iteratively refines rubric text from score mismatches between LLM predictions and human labels, and model's chain-of-thought rationales. We provide the full algorithm in Appendix~\ref{sec:appendix}.

\subsection{Preliminaries}
Let \(\mathcal{D}_{\mathrm{train}}=\{(x_i,y_i)\}_{i=1}^{N}\) be the training set, where \(x_i\) is an essay response and \(y_i\) is the corresponding human score. Rubric search starts from an initial seed rubric \(\rubric_0\) (from coarse to detailed variants) and iteratively updates it over \(T\) iterations, maintaining a candidate pool of at most \(K\) rubrics. At each iteration, \(M\) Monte Carlo trials are run per candidate, each drawing error batches of sizes \(b \in \mathcal{B}\).

Given a rubric \(\rubric\), the evaluator LLM scores each training essay \(x_i\), producing a predicted score \(\hat{y}_i\) and a chain-of-thought rationale \(z_i\):
\begin{equation}
(\hat{y}_i,\, z_i) = \mathrm{LLM}(\rubric,\, x_i).
\end{equation}
We measure rubric quality by Quadratic Weighted Kappa (QWK) between the predicted scores \(\hat{\mathbf{y}}_{\rubric}=(\hat{y}_1,\dots,\hat{y}_N)\) and the human scores \(\mathbf{y}=(y_1,\dots,y_N)\), and seek:
\begin{equation}
\rubric_{\mathrm{best}} = \arg\max_{\rubric}\;\mathrm{QWK}(\hat{\mathbf{y}}_{\rubric},\,\mathbf{y}).
\end{equation}
For brevity, we hereafter write \(\mathrm{QWK}(\rubric)\) to denote \(\mathrm{QWK}(\hat{\mathbf{y}}_{\rubric},\mathbf{y})\) on the training set. QWK is a standard metric in automated essay scoring for measuring agreement with human raters~\citep{ijcai2019p879}. It is an agreement metric for ordinal labels and penalizes larger score gaps more heavily than smaller ones:
\begin{equation}
\mathrm{QWK} = 1 - \frac{\sum_{i,j} w_{ij} O_{ij}}{\sum_{i,j} w_{ij} E_{ij}}, \quad
w_{ij}=\frac{(i-j)^2}{(L-1)^2},
\end{equation}
where \(O_{ij}\) and \(E_{ij}\) are observed and expected confusion counts, and \(L\) is the number of score levels. QWK ranges from \(-1\) to \(1\) and higher QWK is better: \(1\) indicates perfect agreement, \(0\) indicates chance-level agreement, and \(-1\) indicates complete disagreement.

\subsection{Reflect-and-Revise}
At iteration \(t\), we maintain a candidate pool \(\mathcal{C}_{t-1}\) of size at most \(K\). For each candidate rubric \(\rubric \in \mathcal{C}_{t-1}\), we first score all training samples and collect failed examples:
\begin{equation}
\mathcal{E}(\rubric)=\{(x_i,y_i,\hat{y}_i,z_i)\mid \hat{y}_i \neq y_i\}.
\end{equation}

For each Monte Carlo trial \(m\in\{1,\dots,M\}\) and each batch size \(b \in \mathcal{B}\), we draw a balanced subsample \(\tilde{\mathcal{E}} \subset \mathcal{E}(\rubric)\) of size \(b\), stratified across score levels so that each human-score value is represented as equally as possible. The model then rewrites the rubric by reflecting on the sampled errors and their rationales:
\begin{equation}
\rubric' = \procname{ReviseRubric}(\rubric, \tilde{\mathcal{E}}).
\end{equation}
Concretely, the revision prompt presents the current rubric together with each error case in \(\tilde{\mathcal{E}}\)---including the essay text, the model's predicted score and rationale, and the human score---and asks the model to identify systematic scoring-error patterns and propose targeted rubric modifications (see Appendix~\ref{sec:appendix_prompts} for the full prompt templates).

\subsection{Iterative Update}
Let \(\mathcal{N}_t\) be the union of all newly revised rubrics and the previous top candidates \(\mathcal{C}_{t-1}\). Every rubric in \(\mathcal{N}_t\) is re-evaluated on \(\mathcal{D}_{\mathrm{train}}\), and we retain the top \(K\) by QWK:
\begin{equation}
\mathcal{C}_t=\mathrm{TopK}_{\rubric\in\mathcal{N}_t}\ \mathrm{QWK}(\rubric).
\end{equation}
The global best rubric \(\rubric_{\mathrm{best}}\) is updated only when the best candidate of \(\mathcal{C}_t\) improves over the previous best training QWK. We repeat this process for \(T\) iterations and return \(\rubric_{\mathrm{best}}\).

\section{Experiments}
\label{sec:experiments}

\subsection{Datasets}
We evaluate on three essay scoring benchmarks. The Automated Student Assessment Prize (ASAP) dataset~\citep{ben2012asap} consists of student essays from U.S.\ standardized tests; we use essay set~1 (P1), which contains persuasive essays scored on an integer scale from 1 to 6 by human raters. ASAP 2.0~\citep{Crossley2025ASAP2} is a corpus of source-based argumentative essays written by U.S.\ secondary students across seven prompts, scored on a 1--6 integer scale with consistent rubrics and accompanying source texts; we use the ``Exploring Venus'' subset in our experiments. The TOEFL11 corpus~\citep{blanchard2013toefl} contains English essays written by non-native speakers across eight essay prompts, labeled at three proficiency levels (high, medium, low). For each dataset, we use 100 training samples for rubric refinement and evaluate on 100 held-out test samples. The expert and simplest seed rubrics of ASAP are provided in Appendix~\ref{sec:appendix_rubrics}.

For TOEFL11, the original rubric defines five proficiency levels (scores 1--5), but the dataset labels use three levels (high, medium, low). We adopt the rubric descriptions for score 4 as \textit{high} (mapped to score 3), score 3 as \textit{medium} (mapped to score 2), and score 2 as \textit{low} (mapped to score 1). 

\subsection{Experimental Setup}
We compare our Reflect-and-Revise method against two baselines. The first is the \textbf{Human Rubric} baseline, which directly uses the original human-authored rubric without any refinement. The second is \textbf{AutoCalibrate}, which follows AutoCalibrate~\citep{liu-etal-2024-calibrating} in performing a single-pass rubric revision using score mismatches between model predictions and human labels, without incorporating the model's chain-of-thought rationales. As shown in \autoref{tab:rubric_rewrite_data_requirements}, AutoCalibrate uses the most signals among prior methods (human-labeled scores, evaluated texts, source passages, and model-predicted scores), making it the closest baseline to our approach. We chose AutoCalibrate as our primary baseline also because many rubric refinement methods discussed in \autoref{tab:rubric_rewrite_data_requirements} incorporate post-processing steps beyond rubric modification (e.g., score calibration or aggregation), making it difficult to isolate the effect of rubric refinement itself. To further ensure a fair comparison, we initialize AutoCalibrate from the same human-authored rubric as our method, rather than generating an initial rubric from scratch with an LLM as in the original work.

We evaluate using three frontier LLMs accessed via the OpenRouter API~\citep{openrouter}: \texttt{GPT-5-mini}~\citep{gpt5}, \texttt{Gemini~3~Flash}~\citep{Google2025GeminiFlash}, and \texttt{Qwen3-Next-80B-A3B-Instruct}~\citep{qwen3-next}. 
We report QWK between model predictions and human scores on the held-out test set in a zero-shot setting. We experiment with two seed rubric variants: an \textit{expert} rubric, which is the full human-authored rubric provided with the dataset, and a \textit{simplest} rubric, which is a minimal instruction specifying only the score scale (e.g., ``Based on the response's content, rate the response on a scale of 1 to 6.''). Detailed hyperparameters, including model-specific generation parameters, are provided in Appendix~\ref{sec:appendix_hparams}.

\section{Experimental Results}
\label{sec:results}

\subsection{Main Results}
\label{sec:main_results}
Results on ASAP, ASAP 2.0 and TOEFL11 are provided in Tables~\ref{tab:asap1},~\ref{tab:asap2} and~\ref{tab:toefl} respectively. Bold and underlined values indicate the best and second-best scores for each model, respectively.

\begin{table*}[t]
\centering
\begin{tabular}{llrrrrr}
\toprule
LLM & Rubric written by & QWK & Accuracy & Spearman & Macro-F1 & MAE \\
\midrule
Gemini 3 Flash & Human & 0.427 & 0.404 & 0.568 & 0.307 & 0.697 \\
Gemini 3 Flash & AutoCalibrate & 0.489 & 0.590 & 0.493 & \underline{0.382} & 0.460 \\
Gemini 3 Flash & \textbf{Ours} & \textbf{0.646} & \textbf{0.680} & \textbf{0.656} & \textbf{0.595} & \textbf{0.330} \\
Gemini 3 Flash & \textbf{Ours from simplest} & \underline{0.580} & \underline{0.650} & \underline{0.619} & 0.338 & \underline{0.360} \\
\midrule
GPT-5 mini & Human & 0.042 & 0.110 & 0.151 & 0.058 & 1.430 \\
GPT-5 mini & AutoCalibrate & \underline{0.390} & \underline{0.410} & \underline{0.396} & \underline{0.231} & \underline{0.620} \\
GPT-5 mini & \textbf{Ours} & \textbf{0.480} & \textbf{0.450} & \textbf{0.510} & \textbf{0.271} & \textbf{0.600} \\
GPT-5 mini & \textbf{Ours from simplest} & 0.314 & 0.360 & 0.282 & 0.219 & 0.700 \\
\midrule
Qwen3-80B-A3B & Human & 0.121 & 0.130 & 0.323 & 0.123 & 1.540 \\
Qwen3-80B-A3B & AutoCalibrate & \underline{0.240} & 0.310 & \underline{0.458} & \underline{0.233} & 1.100 \\
Qwen3-80B-A3B & \textbf{Ours} & \textbf{0.473} & \textbf{0.520} & \textbf{0.490} & \textbf{0.402} & \textbf{0.570} \\
Qwen3-80B-A3B & \textbf{Ours from simplest} & 0.154 & \underline{0.320} & 0.188 & 0.158 & \underline{0.920} \\
\bottomrule
\end{tabular}
\caption{Zero-shot evaluation results on ASAP. Bold and underlined values indicate the best and second-best scores for each model.}
\label{tab:asap1}
\end{table*}

\begin{table*}[t]
\centering
\begin{tabular}{llrrrrr}
\toprule
LLM & Rubric written by & QWK & Accuracy & Spearman & Macro-F1 & MAE \\
\midrule
Gemini 3 Flash & Human & 0.613 & 0.440 & 0.657 & 0.270 & 0.610 \\
Gemini 3 Flash & AutoCalibrate & 0.646 & \underline{0.490} & \textbf{0.716} & 0.297 & \underline{0.550} \\
Gemini 3 Flash & \textbf{Ours} & \textbf{0.700} & \textbf{0.540} & \underline{0.716} & \textbf{0.393} & \textbf{0.520} \\
Gemini 3 Flash & \textbf{Ours from simplest} & \underline{0.651} & 0.470 & 0.682 & \underline{0.310} & 0.580 \\
\midrule
GPT-5 mini & Human & 0.358 & \underline{0.350} & \underline{0.533} & 0.148 & 0.780 \\
GPT-5 mini & AutoCalibrate & 0.338 & 0.340 & 0.514 & 0.142 & 0.790 \\
GPT-5 mini & \textbf{Ours} & \textbf{0.537} & \textbf{0.440} & \textbf{0.618} & \textbf{0.239} & \textbf{0.620} \\
GPT-5 mini & \textbf{Ours from simplest} & \underline{0.452} & \underline{0.350} & 0.454 & \underline{0.176} & \underline{0.750} \\
\midrule
Qwen3-80B-A3B & Human & 0.575 & 0.270 & \textbf{0.668} & 0.272 & 0.890 \\
Qwen3-80B-A3B & AutoCalibrate & \underline{0.595} & \underline{0.290} & 0.643 & \textbf{0.310} & 0.850 \\
Qwen3-80B-A3B & \textbf{Ours} & \textbf{0.636} & \textbf{0.410} & \underline{0.645} & \underline{0.295} & \textbf{0.660} \\
Qwen3-80B-A3B & \textbf{Ours from simplest} & 0.509 & \underline{0.290} & 0.558 & 0.214 & \underline{0.820} \\
\bottomrule
\end{tabular}
\caption{Zero-shot evaluation results on ASAP 2.0. Bold and underlined values indicate the best and second-best scores for each model.}
\label{tab:asap2}
\end{table*}

\begin{table*}[t]
\centering
\begin{tabular}{llrrrrr}
\toprule
LLM & Rubric written by & QWK & Accuracy & Spearman & Macro-F1 & MAE \\
\midrule
Gemini 3 Flash & Human & \underline{0.574} & 0.690 & \underline{0.621} & \underline{0.636} & 0.320 \\
Gemini 3 Flash & AutoCalibrate & 0.510 & 0.650 & 0.528 & 0.591 & 0.360 \\
Gemini 3 Flash & \textbf{Ours} & 0.557 & \underline{0.720} & 0.568 & 0.630 & \underline{0.280} \\
Gemini 3 Flash & \textbf{Ours from simplest} & \textbf{0.663} & \textbf{0.770} & \textbf{0.672} & \textbf{0.712} & \textbf{0.230} \\
\midrule
GPT-5 mini & Human & \underline{0.447} & \underline{0.680} & \underline{0.475} & 0.528 & \underline{0.320} \\
GPT-5 mini & AutoCalibrate & \textbf{0.539} & \textbf{0.730} & \textbf{0.580} & \textbf{0.578} & \textbf{0.270} \\
GPT-5 mini & \textbf{Ours} & 0.348 & 0.650 & 0.367 & 0.493 & 0.360 \\
GPT-5 mini & \textbf{Ours from simplest} & 0.394 & 0.640 & 0.391 & \underline{0.545} & 0.370 \\
\midrule
Qwen3-80B-A3B & Human & 0.364 & 0.470 & \underline{0.476} & 0.423 & 0.560 \\
Qwen3-80B-A3B & AutoCalibrate & \underline{0.456} & 0.580 & 0.464 & \textbf{0.545} & 0.440 \\
Qwen3-80B-A3B & \textbf{Ours} & 0.452 & \underline{0.590} & 0.468 & 0.516 & \underline{0.420} \\
Qwen3-80B-A3B & \textbf{Ours from simplest} & \textbf{0.493} & \textbf{0.600} & \textbf{0.502} & \underline{0.544} & \textbf{0.400} \\
\bottomrule
\end{tabular}
\caption{Zero-shot evaluation results on TOEFL11. Bold and underlined values indicate the best and second-best scores for each model.}
\label{tab:toefl}
\end{table*}

\subsection{Refinement from Minimal Seed Rubrics}
\label{sec:simplest_seed}
A key practical question is whether rubric refinement can reduce the burden of manual rubric authoring for LLMs. To investigate this, we apply our method starting from a \textit{simplest} seed rubric and compare the resulting test QWK against the unrefined human expert rubric. Table~\ref{tab:simplest} reports these results.

\begin{table}[t]
\centering
\small
\resizebox{\columnwidth}{!}{%
\begin{tabular}{llrr}
\toprule
\textbf{Dataset} & \textbf{LLM} & \textbf{Ours from} & \textbf{$\Delta$ vs.\ expert} \\
 & & \textbf{simplest} & \textbf{rubric} \\
\midrule
ASAP & GPT-5 mini & 0.314 & +0.272 \\
 & Gemini 3 Flash & 0.580 & +0.153 \\
 & Qwen3-80B-A3B & 0.154 & +0.034 \\
\midrule
ASAP 2.0 & GPT-5 mini & 0.452 & +0.094 \\
 & Gemini 3 Flash & 0.651 & +0.038 \\
 & Qwen3-80B-A3B & 0.509 & $-$0.066 \\
\midrule
TOEFL11 & GPT-5 mini & 0.394 & $-$0.053 \\
 & Gemini 3 Flash & 0.663 & +0.088 \\
 & Qwen3-80B-A3B & 0.493 & +0.129 \\
\bottomrule
\end{tabular}%
}
\caption{QWK comparison: rubrics refined from the simplest seed (``Based on the response's content, rate the response on a scale of 1 to 6.'') vs.\ the rubric written by human experts. $\Delta$ denotes QWK improvement over the baseline using human expert rubrics. Our method achieves comparable or better performance than the human expert rubric in most settings, demonstrating that iterative refinement can substantially reduce the need for manual rubric authoring.}
\label{tab:simplest}
\end{table}

\section{Analysis}
\label{sec:analysis}

\paragraph{Quantitative and Qualitative Analysis of Refined Rubrics}
\label{sec:qualitative}
% To understand how refinement changes rubric content

\paragraph{Iterative Refinement and Monte Carlo Effects}
To analyze the effect of iterative refinement and Monte Carlo trials, which controls depth and breadth of exploration, respectively, we track the best QWK in training dataset at each iteration. Figure~\ref{fig:training_qwk} shows these trajectories of Gemini 3 Flash on ASAP 2.0. By using large number of Monte Carlo trials, our method explores a wide variety of rubric modifications at each iteration, which leads to steady improvements in training QWK across iterations for all three models. 

About gains from iteration, we calculate the stepwise improvement in QWK at each iteration, defined as the difference in best training QWK between steps. Figure~\ref{fig:qwk_gains} plots these stepwise improvements across iterations for all three datasets averaging across models. The largest gains tend to occur in early iterations, with diminishing returns in later iterations.

\begin{figure}[t]
\centering
\includegraphics[width=0.9\linewidth]{figures/fig_iterative_refinement_mc_ASAP2.pdf}
\caption{Best training QWK across iterations of Gemini 3 Flash on ASAP 2.0. MC stands for Monte Carlo trials. By using a large number of Monte Carlo trials, our method explores a wide variety of rubric modifications at each iteration, which leads to improvements in training QWK across iterations.}
\label{fig:training_qwk}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.9\linewidth]{figures/fig_stepwise_improvement.pdf}
\caption{Best training QWK improvement between refinement steps, averaged across models for each dataset. $s_{t-n}\!\to\!s_{t}$ denotes the relative improvement from step $t-n$ to step $t$. The largest gains tend to occur in early iterations.}
\label{fig:qwk_gains}
\end{figure}

\paragraph{Ablation on Iterative and Rationale Components}
To quantify the contribution of iterative refinement and chain-of-thought rationales, we perform an ablation study using ASAP 2.0 dataset. We compare our full method against two ablated variants: (1) \textbf{w/o Iteration}, which performs only a single revision step (i.e., \(T=1\)) without further iterations, and (2) \textbf{w/o Rationale}, which performs iterative refinement but omits the model's chain-of-thought rationales from the revision prompt, relying solely on score mismatches as in AutoCalibrate. Table~\ref{tab:ablation} reports the QWK results for these variants. The full method outperforms both ablated variants across all three models, confirming that both iterative refinement and chain-of-thought rationales contribute to performance improvements.

\begin{table}[t]
\centering
\small
\begin{tabular}{llr}
\toprule
LLM & Method & QWK \\
\midrule
Gemini 3 Flash & Ours & \textbf{0.700} \\
Gemini 3 Flash & w/o Iteration ($T=1$) & 0.616 \\
Gemini 3 Flash & w/o Rationale & \underline{0.665} \\
\midrule
GPT-5 mini & Ours & \textbf{0.537} \\
GPT-5 mini & w/o Iteration ($T=1$) & 0.325 \\
GPT-5 mini & w/o Rationale & \underline{0.527} \\
\midrule
Qwen3-80B-A3B & Ours & \textbf{0.636} \\
Qwen3-80B-A3B & w/o Iteration ($T=1$) & 0.522 \\
Qwen3-80B-A3B & w/o Rationale & \underline{0.561} \\
\bottomrule
\end{tabular}
\caption{The full method (Ours) outperforms both ablated variants, confirming the importance of both iterative refinement and chain-of-thought rationales for maximizing rubric effectiveness.}
\label{tab:ablation}
\end{table}

\paragraph{Scoring distribution analysis}
To visualize how rubric refinement changes scoring behavior, Figure~\ref{fig:confusion} shows confusion matrices for Qwen3-80B-A3B with the human expert rubric versus our refined rubric across all three datasets. On ASAP~P1, the human rubric causes severe under-scoring: predictions cluster around scores 2--3 regardless of the true score. Our refined rubric shifts the distribution toward the diagonal, concentrating predictions around score~4---the most frequent true score---and dramatically improving accuracy from 13\% to 40\%. On ASAP~2.0, a similar pattern emerges: the human rubric scatters predictions across scores 1--3, while our rubric aligns predictions with the true score distribution, improving accuracy from 27\% to 51\%. On TOEFL11, the human rubric never predicts the ``High'' category, collapsing all predictions into Low and Medium. Our refined rubric recovers the ability to predict all three levels, improving accuracy from 48\% to 61\%, although some Low-scored essays are misclassified as Medium. These patterns suggest that the primary mechanism of improvement is better calibration of score boundaries to match the target distribution.

% \paragraph{Cross-model rubric transferability}
% An important question for practical deployment is whether rubrics optimized for one model can benefit other models. To investigate this, we evaluate rubrics refined by each model on the other two models. Table~\ref{tab:cross_model} shows cross-model QWK for expert-seed rubrics refined with our method.

% \begin{table*}[t]
% \centering
% \small
% \begin{tabular}{lrrr}
% \toprule
% \textbf{Source $\backslash$ Evaluator} & \textbf{GPT-5 mini} & \textbf{Gemini 3 Flash} & \textbf{Qwen3-80B-A3B} \\
% \multicolumn{4}{l}{\textit{ASAP P1}} \\
% Human & 0.042 & 0.427 & 0.121 \\
% GPT-5 mini & \textbf{\cellcolor{gray!20}0.429} & 0.424 & \textbf{0.369} \\
% Gemini 3 Flash & 0.235 & \textbf{\cellcolor{gray!20}0.643} & 0.355 \\
% Qwen3-80B-A3B & 0.060 & 0.297 & \cellcolor{gray!20}0.339 \\
% \midrule
% \multicolumn{4}{l}{\textit{ASAP 2.0}} \\
% Human & 0.358 & 0.613 & \textbf{0.575} \\
% GPT-5 mini & \cellcolor{gray!20}0.398 & \textbf{0.651} & 0.329 \\
% Gemini 3 Flash & \textbf{0.399} & \cellcolor{gray!20}0.639 & 0.312 \\
% Qwen3-80B-A3B & 0.321 & 0.644 & \cellcolor{gray!20}0.558 \\
% \midrule
% \multicolumn{4}{l}{\textit{TOEFL11}} \\
% Human & 0.447 & \textbf{0.574} & 0.364 \\
% GPT-5 mini & \textbf{\cellcolor{gray!20}0.485} & 0.557 & 0.341 \\
% Gemini 3 Flash & 0.360 & \cellcolor{gray!20}0.477 & 0.483 \\
% Qwen3-80B-A3B & 0.107 & 0.376 & \textbf{\cellcolor{gray!20}0.499} \\
% \bottomrule
% \end{tabular}
% \caption{Cross-model rubric transferability (QWK). Each cell shows QWK when the rubric optimized by the \textit{source} model (row) is used by the \textit{evaluator} model (column). Shaded diagonal cells denote same-model results. ``Human'' row shows the unrefined human rubric baseline. Bold values indicate the best QWK per evaluator model within each dataset.}
% \label{tab:cross_model}
% \end{table*}

% The results reveal that transferability is highly asymmetric and dataset-dependent. On ASAP P1, the rubric refined by Gemini 3 Flash yields the best QWK not only for Gemini itself (0.576) but also for Qwen3-80B-A3B (0.524), substantially outperforming Qwen's own refined rubric (0.213). Similarly, on ASAP 2.0, the rubric refined by GPT-5 mini achieves the highest QWK when used by Gemini 3 Flash (0.688), surpassing Gemini's own rubric (0.639). These cases suggest that some models produce rubrics that capture more generalizable scoring criteria. However, the reverse transfers are often ineffective: on ASAP P1, the Qwen-refined rubric yields near-zero QWK for GPT-5 mini (0.001). On TOEFL11, the unrefined human rubric remains the strongest option for GPT-5 mini and Gemini 3 Flash, indicating that refined rubrics can overfit to model-specific scoring patterns on certain datasets. Overall, while cross-model transfer can sometimes be beneficial, rubrics are generally most effective when optimized for the target model.

\section{Conclusion}

We proposed an iterative Reflect-and-Revise approach for refining scoring rubrics in LLM-based automated essay scoring. By prompting models to reflect on their own chain-of-thought rationales and observed score discrepancies with human labels, our method enables targeted rubric revisions that improve alignment with human scoring. Experiments on three datasets and three models demonstrate that our approach yields substantial QWK improvements on ASAP P1 (up to +0.388) and consistent gains on ASAP 2.0, while also showing that rubrics refined from minimal seed specifications can match or exceed human expert rubrics. However, the effectiveness varies across datasets, with limited or negative gains on TOEFL11 for some models. These findings suggest that iterative rubric refinement is a promising direction for reducing manual rubric authoring effort and improving LLM-based AES, while also highlighting the need for future work on preventing overfitting to training distributions and improving cross-dataset robustness.

\section*{Limitations}

Our study has several limitations. First, our evaluation covers three essay scoring benchmarks with a single essay set per dataset. The generalizability of our findings to other scoring tasks, writing genres, or educational contexts remains to be verified through broader evaluation.

Second, the effectiveness of our method varies considerably across datasets and models. On TOEFL11, the refined rubrics underperform the original human rubric for some models, suggesting that the refinement process can introduce biases that do not generalize beyond the training samples. This inconsistency indicates that our method may be less effective when the initial rubric is already well-calibrated for the target model or when the training and test distributions differ substantially.

Third, the iterative refinement process incurs non-trivial computational cost, as each iteration requires scoring all training samples with the current rubric candidates. With five iterations, three candidates, and multiple batch sizes, the total number of LLM calls scales considerably, which may limit practical applicability in resource-constrained settings.

Fourth, our experiments use only 100 training samples for rubric refinement. While this reflects realistic scenarios where labeled data is scarce, the small sample size may increase the risk of overfitting the rubric to idiosyncratic properties of the training set rather than capturing generalizable scoring patterns.

Finally, our cross-model transfer analysis reveals that rubrics optimized for one model do not reliably benefit other models, and in some cases substantially degrade performance. This limits the reusability of refined rubrics and suggests that separate optimization may be required for each target model.

\bibliography{custom}
\appendix

\section{Algorithm}
\label{sec:appendix}

Algorithm~\ref{alg:rubric_refinement} gives the full pseudocode for iterative rubric refinement.

\begin{algorithm*}[t]
\caption{Iterative Rubric Refinement}
\label{alg:rubric_refinement}
\begin{algorithmic}[1]
\Require Training set $\mathcal{D}_{\mathrm{train}} = \{(x_i, y_i)\}_{i=1}^{N}$, seed rubric $\rubric_0$, iterations $T$, pool size $K$, Monte Carlo trials $M$, batch sizes $\mathcal{B}$, evaluator LLM
\Ensure Best rubric $\rubric_{\mathrm{best}}$
\State $\mathcal{C}_0 \gets \{\rubric_0\}$;\; $\rubric_{\mathrm{best}} \gets \rubric_0$;\; $q_{\mathrm{best}} \gets \mathrm{QWK}(\rubric_0)$
\For{$t = 1$ \textbf{to} $T$}
    \State $\mathcal{N}_t \gets \emptyset$
    \For{\textbf{each} $\rubric \in \mathcal{C}_{t-1}$}
        \State Score all $(x_i,y_i) \in \mathcal{D}_{\mathrm{train}}$ with $\rubric$ $\to$ $(\hat{y}_i, z_i)$
        \State $\mathcal{E}(\rubric) \gets \{(x_i,y_i,\hat{y}_i,z_i) \mid \hat{y}_i \neq y_i\}$
        \For{$m = 1$ \textbf{to} $M$}
            \For{\textbf{each} $b \in \mathcal{B}$}
                \State $\tilde{\mathcal{E}} \gets \procname{BalancedSample}(\mathcal{E}(\rubric),\, b)$
                % \Comment{Stratified by score level}
                \State $\rubric' \gets \procname{ReviseRubric}(\rubric,\, \tilde{\mathcal{E}})$
                % \Comment{LLM revises rubric}
                \State $\mathcal{N}_t \gets \mathcal{N}_t \cup \{\rubric'\}$
            \EndFor
        \EndFor
    \EndFor
    \State $\mathcal{N}_t \gets \mathcal{N}_t \cup \mathcal{C}_{t-1}$
    % \Comment{Retain previous top candidates}
    \State Re-evaluate all $\rubric \in \mathcal{N}_t$ on $\mathcal{D}_{\mathrm{train}}$
    \State $\mathcal{C}_t \gets \mathrm{TopK}_{\rubric \in \mathcal{N}_t}\;\mathrm{QWK}(\rubric)$
    \If{$\max_{\rubric \in \mathcal{C}_t} \mathrm{QWK}(\rubric) > q_{\mathrm{best}}$}
        \State $\rubric_{\mathrm{best}} \gets \arg\max_{\rubric \in \mathcal{C}_t} \mathrm{QWK}(\rubric)$;\; $q_{\mathrm{best}} \gets \mathrm{QWK}(\rubric_{\mathrm{best}})$
    \EndIf
\EndFor
\State \Return $\rubric_{\mathrm{best}}$
\end{algorithmic}
\end{algorithm*}

\procname{BalancedSample}$(\mathcal{E}, b)$ partitions the error set $\mathcal{E}$ into buckets by human-score level, then draws $\lfloor b / L' \rfloor$ examples from each bucket (where $L'$ is the number of non-empty buckets), ensuring that each score level is represented as equally as possible.

\section{Prompt Templates}
\label{sec:appendix_prompts}

This section lists the exact prompt templates used in our pipeline. Placeholders enclosed in braces (e.g., \texttt{\{rubric\}}) are filled at runtime.

\subsection{Evaluation Prompt}
\label{sec:eval_prompt}
The following prompt is used to score each essay with the current rubric. The LLM is instructed to output a rationale followed by an integer score.

\begin{tcolorbox}[colback=white, colframe=RoyalBlue, title=Evaluation Prompt, fonttitle=\bfseries\small, fontupper=\small, breakable]
You are an expert rater for a high-stakes English writing exam for second-language learners.
Evaluate the response strictly using the scoring guideline. Choose exactly one score from the scoring guideline's score points.

\texttt{\# Essay Prompt}\\
\texttt{"""\{essay\_prompt\}"""}

\texttt{\# Response}\\
\texttt{"""\{response\}"""}

\texttt{\# Scoring Guideline}\\
\texttt{"""\{rubric\}"""}

\texttt{\# Output format (follow exactly)}\\
\texttt{Rationale: [<<<Brief evidence-based rationale.>>>]}\\
\texttt{Rating: [<<<One integer score only.>>>]}
\end{tcolorbox}

\subsection{Error-Case Format for Rubric Revision}
\label{sec:error_format}
For each incorrectly scored example fed into the revision prompt, the following format is used to present the error case to the model. When rationales are included (\texttt{with\_rationale=True}), the model's chain-of-thought is shown alongside its predicted score.

\begin{tcolorbox}[colback=white, colframe=RoyalBlue, title=Error-Case Format (with rationale), fonttitle=\bfseries\small, fontupper=\small, breakable]
\texttt{Assistant input:}\\
\texttt{Essay prompt:}\\
\texttt{"""\{essay\_prompt\}"""}\\
\texttt{Essay to be rated:}\\
\texttt{"""\{response\}"""}\\
\texttt{Assistant rationale:}\\
\texttt{"""\{rationale\}"""}\\
\texttt{Assistant score:}\\
\texttt{"""\{rating\}"""}\\
\texttt{Desired score:}\\
\texttt{"""\{desired\_rating\}"""}
\end{tcolorbox}

\subsection{Rubric Revision Prompt}
\label{sec:revision_prompt}
The revision prompt wraps the current rubric and the sampled error cases, and instructs the model to output a revised rubric.

\begin{tcolorbox}[colback=white, colframe=RoyalBlue, title=Rubric Revision Prompt (with rationale), fonttitle=\bfseries\small, fontupper=\small, breakable]
I asked an assistant to grade essays using the scoring guideline below:

\texttt{```}\\
\texttt{\{current\_rubric\}}\\
\texttt{```}

Here are grading examples that include the assistant input, the assistant rationale, the assistant score, and the desired score:

\texttt{```}\\
\texttt{\{examples\}}\\
\texttt{```}

Revise the scoring guideline to improve score agreement so the assistant's future ratings align more closely with the desired scores.

Requirements:\\
1. Use the rationale patterns to identify why the assistant over-scored or under-scored, and improve the scoring guideline guidance accordingly to reduce score mismatches.

Output rules:\\
-- Return only the revised scoring guideline.\\
-- Use exactly one fenced code block with triple backticks.\\
-- Do not include any text before or after the code block.
\end{tcolorbox}

When rationales are not used (\texttt{with\_rationale=False}), the error-case format omits the \texttt{Assistant rationale} field, and the requirement in the revision prompt is replaced with: ``Use score mismatch patterns to identify where the scoring guideline guidance is insufficient or ambiguous, and revise it to reduce score mismatches.''

\section{Hyperparameters}
\label{sec:appendix_hparams}

Table~\ref{tab:hyperparams} summarizes the optimization hyperparameters. We adopt the number of Monte Carlo trials ($M{=}4$) from AutoCalibrate. For error batch sizes, AutoCalibrate originally used $\mathcal{B}{=}\{1,2,4\}$, but considering the improved capacity of recent LLMs to process longer contexts, we increase them to $\mathcal{B}{=}\{4,8,12\}$. For the number of iterations, the original self-refinement loop in \citet{madaan2023selfrefine} used $T{=}4$; we set $T{=}5$, expecting that the stronger reasoning and instruction-following capabilities of recent models would allow productive refinement over additional cycles. The AutoCalibrate baseline uses $T{=}1$ (single-pass revision) without rationales, while our Reflect-and-Revise method performs iterative refinement with rationale-based feedback. Both methods maintain $K{=}3$ top candidates and use $N{=}100$ training samples.

\begin{table}[h]
  \centering
  \small
  \begin{tabular}{lcc}
    \toprule
    \textbf{Hyperparameter} & \textbf{Ours} & \textbf{AutoCalibrate} \\
    \midrule
    Iterations ($T$) & 5 & 1 \\
    Top-$K$ candidates & 3 & 3 \\
    Monte Carlo trials ($M$) & 4 & 4 \\
    Batch sizes ($\mathcal{B}$) & \{4, 8, 12\} & \{4, 8, 12\} \\
    Training samples ($N$) & 100 & 100 \\
    With rationale & Yes & No \\
    \bottomrule
  \end{tabular}
  \caption{Optimization hyperparameters for rubric refinement.}
  \label{tab:hyperparams}
\end{table}

For all models, we set the maximum output tokens to 8192. For GPT-5-mini and Gemini 3 Flash, we set reasoning effort to low. All other generation parameters were left at their default values for each model.


\section{Seed Rubrics}
\label{sec:appendix_rubrics}

This section lists the expert (human-authored) and simplest seed rubrics of the ASAP dataset used in our experiments. The expert rubric is the original human-authored rubric provided with the dataset, while the simplest rubric is a minimal description specifying only the score scale.

\subsection{Expert rubric}

\begin{tcolorbox}[colback=white, colframe=gray, fontupper=\small, breakable]
Score Point 1: An undeveloped response that may take a position but offers no more than very minimal support. Typical elements:
- Contains few or vague details.
- Is awkward and fragmented.
- May be difficult to read and understand.
- May show no awareness of audience.

Score Point 2: An under-developed response that may or may not take a position. Typical elements:
- Contains only general reasons with unelaborated and/or list-like details.
- Shows little or no evidence of organization.
- May be awkward and confused or simplistic.
- May show little awareness of audience.

Score Point 3: A minimally-developed response that may take a position, but with inadequate support and details. Typical elements:
- Has reasons with minimal elaboration and more general than specific details.
- Shows some organization.
- May be awkward in parts with few transitions.
- Shows some awareness of audience.

Score Point 4: A somewhat-developed response that takes a position and provides adequate support. Typical elements:
- Has adequately elaborated reasons with a mix of general and specific details.
- Shows satisfactory organization.
- May be somewhat fluent with some transitional language.
- Shows adequate awareness of audience.

Score Point 5: A developed response that takes a clear position and provides reasonably persuasive support. Typical elements:
- Has moderately well elaborated reasons with mostly specific details.
- Exhibits generally strong organization.
- May be moderately fluent with transitional language throughout.
- May show a consistent awareness of audience.

Score Point 6: A well-developed response that takes a clear and thoughtful position and provides persuasive support. Typical elements:
- Has fully elaborated reasons with specific details.
- Exhibits strong organization.
- Is fluent and uses sophisticated transitional language.
- May show a heightened awareness of audience.

Note: 
I have made an effort to remove personally identifying information from the essays using the Named Entity Recognizer (NER). The relevant entities are identified in the text and then replaced with a string such as "PERSON", "ORGANIZATION", "LOCATION", "DATE", "TIME", "MONEY", "PERCENT", "CAPS" (any capitalized word) and "NUM" (any digits). Please do not penalize the essay because of the anonymizations.
\end{tcolorbox}

\subsection{Simplest rubric}

\begin{tcolorbox}[colback=white, colframe=gray, fontupper=\small]
Based on the response's content, rate the response on a scale of 1 to 6.
\end{tcolorbox}

\subsection{Refined rubric (GPT-5~mini, ASAP P1, expert seed)}
\label{sec:appendix_refined_rubric}

The following is the full text of the rubric refined by GPT-5~mini on ASAP P1 starting from the expert seed rubric, corresponding to the excerpts shown in Figure~\ref{fig:rubric_pattern_asap_1_openai_gpt_5_mini_base_expert_True_train100_iteration5_top3_bs4_8_12_mc4}.

\begin{tcolorbox}[colback=white, colframe=gray, fontupper=\small, breakable]
Revised Scoring Guideline (with added clarifications to improve rater alignment and reduce common mismatches)

\textbf{General notes} (apply to all scores)
\begin{itemize}
\setlength{\itemsep}{1pt}
\item Do not penalize anonymization (PERSON, ORGANIZATION, LOCATION, DATE, TIME, MONEY, PERCENT, CAPS, NUM).
\item Distinguish surface errors (spelling, grammar, typos) from communicative breakdowns. Surface errors are expected and should only lower a score if they substantially impede meaning, coherence, or readability.
\item Consider four core dimensions when placing a response: Position/claim clarity, Development/Specificity of support, Organization/Coherence/Transitions, and Readability/Fluency. A single weak dimension can lower score by at most one point unless it causes major incomprehensibility.
\item Use ``examples'' to mean concrete, specific instances, anecdotes, or explained scenarios---not mere lists of general categories.
\item When in doubt between adjacent scores, use the development and specificity of examples as the primary tiebreaker: more distinct, explained examples $\to$ higher score.
\end{itemize}

\textbf{New guidance to address recurring rater mismatches}
\begin{itemize}
\setlength{\itemsep}{1pt}
\item Treat brief anecdotes or named personal examples (e.g., ``my neighbor,'' ``a study found,'' short story about a person) as concrete examples even when they are imperfectly written or poorly sourced. Such examples should count toward the essay's level of development, so long as they are distinct and clearly connected to the claim.
\item Multiple concrete examples that are each briefly explained (even if explanations are awkward or contain errors) should generally move a response up one score level relative to essays that only list general reasons. Specifically: multiple distinct examples + brief explanations $\to$ prefer 5 over 4; multiple named anecdotes/examples across domains $\to$ prefer 5 over 4.
\item Repetition of the same point with minor variation does not equal multiple elaborations. Distinctness matters: different domains, different types of evidence, or separate illustrative anecdotes count as multiple examples; restatements do not.
\item Poor mechanics/surface errors alone should not prevent an essay from earning 4 or 5 if the claim, organization, and development meet those criteria. Reserve downgrades for errors only when they create frequent confusion or break comprehension.
\item Organization: Even basic intro/body/conclusion structure with clear grouping of reasons should satisfy the ``organization'' requirement for scores 3--5. Only very fragmented or missing structure should lead to a 1 or 2.
\end{itemize}

\textbf{Score Point 1:} Undeveloped / Minimal communicative content\\
Typical elements (choose 1--3 to justify a 1):
\begin{itemize}
\setlength{\itemsep}{1pt}
\item Takes little or no defensible position, or the position is incomprehensible.
\item Contains extremely few details or only vague, fragmentary statements.
\item Organization is absent; writing may be so fragmented it is difficult to follow.
\item Errors seriously impede understanding.
\end{itemize}
Rater guidance: Assign 1 when the essay provides almost no development and the reader cannot follow an argument beyond a bare claim.

\textbf{Score Point 2:} Under-developed / General or list-like reasons only\\
Typical elements (choose 2--4 to justify a 2):
\begin{itemize}
\setlength{\itemsep}{1pt}
\item May take a position, but support consists primarily of general assertions or list-like reasons with little to no elaboration.
\item Little or no effective organization; ideas may be loosely connected.
\item Language is often awkward or confused; errors frequent but meaning largely recoverable.
\item No real attempt at addressing counterarguments or providing specific examples.
\end{itemize}
Rater guidance: Use 2 for responses that go beyond fragments but still present primarily undeveloped, general claims with minimal cohesion.

\textbf{Score Point 3:} Minimally developed / Some organization, minimal elaboration\\
Typical elements (choose 2--4 to justify a 3):
\begin{itemize}
\setlength{\itemsep}{1pt}
\item Takes a position and supplies one or more reasons, but elaboration is minimal and details are mostly general rather than specific.
\item Shows some organization (intro, body, conclusion or paragraphing) though transitions may be weak and sequencing simplistic.
\item Examples, if present, are few, repetitive, or only loosely connected to claims (e.g., listing benefits without explanation).
\item Errors and awkward phrasing are evident but do not severely block comprehension.
\end{itemize}
Rater guidance: Assign 3 when the essay shows a clear position and basic organizational structure but lacks adequate, specific development. If there are only general reasons and the examples are mainly restatements or repeats, prefer 3 over 4. A couple of short, distinct examples with almost no explanation can still be 3; require at least brief explanation to move above 3.

\textbf{Score Point 4:} Somewhat developed / Adequate support and organization\\
Typical elements (choose 3--5 to justify a 4):
\begin{itemize}
\setlength{\itemsep}{1pt}
\item Takes a clear position and provides adequately elaborated reasons with a mix of general and specific details.
\item Presents multiple distinct supporting points and at least some specific examples or brief explanations (e.g., named applications, short anecdotes, simple illustrative scenarios).
\item Organization is satisfactory: logical sequence, basic use of transitions, and paragraphs that group ideas.
\item Fluency is moderate; errors present but do not significantly impede persuasiveness or clarity.
\item May briefly acknowledge or respond to a counterargument or limitation.
\end{itemize}
Rater guidance: Use 4 when the response is reasonably persuasive and shows adequate breadth of support with at least some specific examples or short elaborations---even if surface errors or awkwardness are frequent. If the essay offers several reasons with a few concrete examples but the examples are not well explained or are uneven in quality, score 4. If organization is weak but there are multiple specific examples with some explanation, prefer 4 over 3.

\textbf{Score Point 5:} Developed / Clear position with persuasive, specific support\\
Typical elements (choose 3--5 to justify a 5):
\begin{itemize}
\setlength{\itemsep}{1pt}
\item Takes a clear, consistent position and provides reasonably persuasive support with mostly specific details and concrete examples.
\item Reasons are moderately well elaborated: cause/effect, implications, or examples explained beyond mere mention.
\item Organization is strong: clear progression of ideas, reasonably consistent paragraph structure, and functional transitions.
\item Writing is generally fluent; surface errors are present but do not hinder persuasiveness or clarity.
\item May include a limited but relevant counterargument and rebuttal.
\end{itemize}
Rater guidance: Assign 5 when the essay offers multiple distinct, concrete examples that are explained or linked to the claim, has clear logical organization, and is persuasive overall despite some errors. Multiple concrete examples across different domains (e.g., health, environment, social/family) each with brief explanation $\to$ prefer 5 rather than 4. Named anecdotes or brief case reports count as concrete examples when they illustrate the claim and are connected to the reasoning.

\textbf{Score Point 6:} Well-developed / Thoughtful, persuasive, and fluent\\
Typical elements (choose 3--5 to justify a 6):
\begin{itemize}
\setlength{\itemsep}{1pt}
\item Takes a clear and thoughtful position and provides fully elaborated reasons with specific, vivid details and well-chosen examples.
\item Exhibits strong organization and coherence: effective introduction, logically developed paragraphs, clear transitions, and a persuasive conclusion.
\item Writing is fluent and displays sophisticated use of language; errors are rare and do not distract.
\item Demonstrates heightened awareness of audience and purpose (tone, rhetorical devices, effective counterargument/refutation).
\end{itemize}
Rater guidance: Reserve 6 for essays that are persuasive and mature in thought and expression---detailed, well-explained examples, clear reasoning, and strong control of language.

\textbf{Additional decision rules to reduce inconsistency}
\begin{itemize}
\setlength{\itemsep}{1pt}
\item Count distinct examples: 0--1 distinct examples $\to$ less likely than 4; 2--3 distinct examples briefly explained $\to$ 4; 3+ distinct examples with brief explanation across different domains or backed by named anecdotes/studies $\to$ 5.
\item Distinctness beats quantity: three distinct, separate examples (different domains or different types of evidence) that are each at least briefly explained should lift an essay to 5 even if writing is error-prone.
\item Repetition detection: if support is largely the same point repeated (e.g., ``computers cause obesity'' restated many times with minor wording changes), do not count as multiple elaborations.
\item Anecdotes: treat properly connected anecdotes as evidence. If an essay includes several personal or named anecdotes that illustrate separate facets of the claim, these count toward higher development.
\item Organization vs.\ development: When organization is weak but multiple specific, relevant examples are present and explained, prefer 4 or 5 (depending on number/explainedness) rather than 3.
\item Surface errors: Frequent mechanical errors by themselves should not block a 4 or 5. Downgrade more for error-driven incomprehensibility than for presence of many errors alone.
\item When uncertain between 3 and 4: ask whether there are any concrete or named examples that go beyond mere lists. If yes $\to$ 4. When uncertain between 4 and 5: count distinct, explained examples across domains; 3+ $\to$ 5.
\end{itemize}

Illustrative heuristics (short mapping):
\begin{itemize}
\setlength{\itemsep}{1pt}
\item Score 3 $\to$ clear claim + several general reasons, little elaboration, weak transitions (e.g., listing benefits or harms with minimal examples).
\item Score 4 $\to$ clear claim + multiple reasons with at least some specific examples or brief illustrations and satisfactory organization; persuasive but not consistently developed.
\item Score 5 $\to$ clear, persuasive claim + multiple distinct, specific examples that are explained and connected to the thesis; strong organization; readers are likely convinced.
\end{itemize}
\end{tcolorbox}

\begin{table}[t]
\centering
\small
\resizebox{\columnwidth}{!}{%
\begin{tabular}{llrrrrr}
\toprule
\textbf{Dataset} & \textbf{LLM} & \textbf{Baseline} & \textbf{$N=10$} & \textbf{$N=20$} & \textbf{$N=50$} & \textbf{$N=100$} \\
\midrule
ASAP & Gemini 3 Flash & 0.427 & 0.382 & 0.267 & 0.538 & \textbf{0.646} \\
 & GPT-5 mini & 0.042 & 0.294 & 0.249 & 0.409 & \textbf{0.480} \\
 & Qwen3-80B-A3B & 0.121 & 0.302 & 0.395 & 0.383 & \textbf{0.473} \\
\midrule
ASAP 2.0 & Gemini 3 Flash & 0.613 & 0.366 & \textbf{0.724} & 0.619 & 0.700 \\
 & GPT-5 mini & 0.358 & 0.322 & \textbf{0.544} & 0.540 & 0.537 \\
 & Qwen3-80B-A3B & 0.575 & 0.426 & 0.616 & 0.557 & \textbf{0.636} \\
\bottomrule
\end{tabular}%
}
\caption{Effect of training sample size ($N$) on test QWK. Baseline uses the human expert rubric without optimization. Bold indicates the best QWK among training sizes for each model.}
\label{tab:train_size}
\end{table}

\input{figures/tab_rubric_comparison}

% \input{figures/rubric_pattern_highlights/fig_rubric_patterns_all}


\end{document}
