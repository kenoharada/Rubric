\colorlet{pcMatch}{red!75!black}
\begin{figure*}[t]
\centering
\begin{tcolorbox}[
colback=white,
colframe=black!25,
title=Pattern Explanations with Random Matched Snippets,
fonttitle=\bfseries\small,
fontupper=\scriptsize,
boxsep=1pt,left=2pt,right=2pt,top=2pt,bottom=2pt]
\textbf{1. Conditional Gating}\par \textit{Category:} Rule Structure\par \textit{What this pattern captures:} Condition-based branching rules (if / when / unless / provided that) that explicitly guide rater decisions under specific circumstances. Refined rubrics tend to add many conditional gates to reduce ambiguity in borderline situations.\par \textit{Typical cues:} if, when, unless, provided that\par \textit{Example 1:} ``... eaking and possible downward adjustment in Step 4. - \textcolor{pcMatch}{\textbf{If}} E = Minor: no reduction. Step 3 - Assess Central Clai ...'' \par \textit{Example 2:} ``... support the claim and the core logic is present, even \textcolor{pcMatch}{\textbf{if}} execution is flawed - demonstrates adequate but incons ...'' \par\vspace{1.2mm}\par \textbf{2. Boundary / Tie-Break}\par \textit{Category:} Rule Structure\par \textit{What this pattern captures:} Rules for resolving borderline cases between adjacent score bands. Includes tie-break procedures, explicit threshold cutoffs, and 'N vs N' comparisons (e.g., '3 vs 4'). Refined rubrics often add detailed boundary-resolution instructions to improve inter-rater agreement.\par \textit{Typical cues:} tie-break, borderline, threshold, N vs N, between adjacent\par \textit{Example 1:} ``... ts - Final\_score = min(candidate, MAX\_POSSIBLE). - \textcolor{pcMatch}{\textbf{Tie-break}} ers (used only if candidate <= MAX\_POSSIBLE and multiple ...'' \par \textit{Example 2:} ``... be present but do not substantially obscure meaning. - \textcolor{pcMatch}{\textbf{Tie-break}} er to promote consistency: If an essay has 2+ distinct ...'' \par\vspace{1.2mm}\par \textbf{3. Stepwise Workflow}\par \textit{Category:} Rule Structure\par \textit{What this pattern captures:} Ordered step-by-step procedures (Step 1, Step 2...) or checklists that structure the scoring process into a reproducible workflow. Optimization tends to transform free-form scoring guidance into structured, sequential procedures for raters to follow.\par \textit{Typical cues:} step N, checklist, workflow, procedure, in order\par \textit{Example 1:} ``... adjustment in Step 4. - If E = Minor: no reduction. \textcolor{pcMatch}{\textbf{Step 3}} - Assess Central Claim (A), Organization (C), Language ...'' \par \textit{Example 2:} ``... le, or anecdote. Core principles (apply holistically, \textcolor{pcMatch}{\textbf{in order}} of priority): 1. Clarity of position (most important). ...'' \par\vspace{1.2mm}\par \textbf{4. Quantitative Threshold}\par \textit{Category:} Rule Structure\par \textit{What this pattern captures:} Numeric cutoffs and quantified criteria (e.g., 'at least 2 facts', '\textasciitilde{}30\% severe errors', '3 reasons') that replace vague qualitative descriptions with concrete numbers. Optimization frequently introduces numeric thresholds where the original rubric used imprecise terms like 'some' or 'several'.\par \textit{Typical cues:} at least, at most, <=, >=, N reasons/examples/sentences, N\%\par \textit{Example 1:} ``... tructure should not exceed Score 2. - Score 4 requires \textcolor{pcMatch}{\textbf{at least}} one concrete example per reason, even if anonymized. S ...'' \par \textit{Example 2:} ``... th adequate elaboration, combining general claims with \textcolor{pcMatch}{\textbf{at least}} one specific, concrete example per reason (e.g., "I us ...'' \par\vspace{1.2mm}\par \textbf{5. Score Cap / Demotion}\par \textit{Category:} Rule Structure\par \textit{What this pattern captures:} Hard constraints that cap the maximum achievable score or forcibly demote ratings when specific conditions are unmet. Examples: 'cannot receive 4 or higher', 'do not award 5', 'downgrade to 2'. Refined rubrics add these guards to prevent systematic over-scoring of essays that superficially appear competent.\par \textit{Typical cues:} cannot be Score, must not receive, do not award, downgrade, demotion\par \textit{Example 1:} ``... the band. 2. If E = Noticeable and candidate is 5, \textcolor{pcMatch}{\textbf{downgrade}} to 4 unless B = Strong with multiple explained example ...'' \par \textit{Example 2:} ``... metimes obscure meaning (reader must infer), the essay \textcolor{pcMatch}{\textbf{cannot be Score}} 3; assign Score 2 or Score 1 depending on frequency an ...'' \par\vspace{1.2mm}\par \textbf{6. Evidence Count Safeguard}\par \textit{Category:} Evidence Handling\par \textit{What this pattern captures:} Rules preventing inflated evidence counts, combining two closely related aspects: (1) anti-mechanical counting - requiring qualitative judgment rather than naive tallying (e.g., 'do not count', 'not mechanically'); (2) repetition / restatement non-counting - treating repeated or rephrased claims as a single piece of evidence (e.g., 'restatement', 'double-count', 'near-duplicate'). Both serve the same goal: preventing over-scoring of essays that superficially list many facts without genuine variety or depth.\par \textit{Typical cues:} do not count, not mechanically, do not equate, not equivalent, repetition, restatement, double-count, rephrasing, near-duplicate\par \textit{Example 1:} ``... 4. Distinguish repetition vs. distinct evidence: - \textcolor{pcMatch}{\textbf{Repetition}} or rephrasing of the same example should not be counte ...'' \par \textit{Example 2:} ``... ghly creative framing and sophisticated voice. - Heavy \textcolor{pcMatch}{\textbf{repetition}} in sentence structure (e.g., "One reason is... Another ...'' \par\vspace{1.2mm}\par \textbf{7. Concrete Exemplification}\par \textit{Category:} Evidence Handling\par \textit{What this pattern captures:} Detects two related signals of rubric specificity: (1) requirements for essays to include concrete examples, anecdotes, or explicit evidence; (2) the rubric itself using illustrative examples (e.g., for example, for instance) to clarify scoring criteria. Refined rubrics frequently replace abstract descriptions with example-rich explanations, making this a strong indicator of rubric practicality improvement.\par \textit{Typical cues:} specific example, concrete example, concrete detail, anecdote, personal experience, explicit evidence, e.g., for example, for instance\par \textit{Example 1:} ``... ndomly inserted and render core claims unintelligible ( \textcolor{pcMatch}{\textbf{e.g.}} , "@CAPS9 he helps you..."), this qualifies as Score 1. ...'' \par \textit{Example 2:} ``... oint of view, showing sophisticated critical thinking ( \textcolor{pcMatch}{\textbf{e.g.}} , analyzing tone, diction, or complex contradictions). ...'' 
\end{tcolorbox}
\caption{Overview of rubric-refinement patterns and representative rubric snippets. For each pattern, we provide a short interpretation and randomly sampled matched spans from refined rubrics; highlighted words indicate the cue expressions that triggered each pattern. }
\label{fig:pattern_explanation}
\end{figure*}
